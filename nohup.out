INFO:     Will watch for changes in these directories: ['/home/python_backend']
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [15389] using StatReload
INFO:     Started server process [15400]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     182.66.219.18:56925 - "OPTIONS /query-pdf/ HTTP/1.1" 200 OK

--- Sentence Matching Process ---
Target Page: 1
Search Phrases: ['Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.']
Similarity Threshold: 0.03

Comparing: 
  Sentence: 'speech and language processing.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.005128205128205128
Comparing: 
  Sentence: 'daniel jurafsky & james h.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.005194805194805195
Comparing: 
  Sentence: 'martin.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.00546448087431694
Comparing: 
  Sentence: 'copyright © 2024.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.005319148936170213
Comparing: 
  Sentence: 'all rights reserved.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.0158311345646438
Comparing: 
  Sentence: 'draft of august 20, 2024.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.005208333333333333
Comparing: 
  Sentence: 'chapter 10 large language models “how much do we know at any time?'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.0
Comparing: 
  Sentence: 'much more, or so i believe, than we know we know.” agatha christie, the moving finger fluent speakers of a language bring an enormous amount of knowledge to bear dur- ing comprehension and production.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.01073345259391771
Comparing: 
  Sentence: 'this knowledge is embodied in many forms, perhaps most obviously in the vocabulary, the rich representations we have of words and their meanings and usage.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.011673151750972763
Comparing: 
  Sentence: 'this makes the vocabulary a useful lens to explore the acquisition of knowledge from text, by both people and machines.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.029288702928870293
Comparing: 
  Sentence: 'estimates of the size of adult vocabularies vary widely both within and across languages.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.017857142857142856
Comparing: 
  Sentence: 'for example, estimates of the vocabulary size of young adult speakers of american english range from 30,000 to 100,000 depending on the resources used to make the estimate and the deﬁnition of what it means to know a word.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.024096385542168676
Comparing: 
  Sentence: 'what is agreed upon is that the vast majority of words that mature speakers use in their day-to-day interactions are acquired early in life through spoken interactions with caregivers and peers, usually well before the start of formal schooling.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.013245033112582781
Comparing: 
  Sentence: 'this active vocabulary (usually on the order of 2000 words for young speakers) is extremely limited compared to the size of the adult vocabulary, and is quite stable, with very few additional words learned via casual conversation beyond this early stage.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.04241435562805873
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'This active
vocabulary (usually on the order of 2000 words for young speakers) is extremely
limited compared to the size of the adult vocabulary, and is quite stable, with very
few additional words learned via casual conversation beyond this early stage.', Similarity: 0.04241435562805873
Comparing: 
  Sentence: 'obvi- ously, this leaves a very large number of words to be acquired by other means.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.03160270880361174
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Obvi-
ously, this leaves a very large number of words to be acquired by other means.', Similarity: 0.03160270880361174
Comparing: 
  Sentence: 'a simple consequence of these facts is that children have to learn about 7 to 10 words a day, every single day, to arrive at observed vocabulary levels by the time they are 20 years of age.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.014598540145985401
Comparing: 
  Sentence: 'and indeed empirical estimates of vocabulary growth in late el- ementary through high school are consistent with this rate.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.008298755186721992
Comparing: 
  Sentence: 'how do children achieve this rate of vocabulary growth?'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.01932367149758454
Comparing: 
  Sentence: 'the bulk of this knowledge acquisition seems to happen as a by-product of reading, as part of the rich processing and reasoning that we perform when we read.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.011627906976744186
Comparing: 
  Sentence: 'research into the average amount of time children spend reading, and the lexical diversity of the texts they read, indicate that it is possible to achieve the desired rate.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.022598870056497175
Comparing: 
  Sentence: 'but the mechanism behind this rate of learning must be remarkable indeed, since at some points during learning the rate of vocabulary growth exceeds the rate at which new words are appearing to the learner!'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.010619469026548672
Comparing: 
  Sentence: 'such facts have motivated the distributional hypothesis of chapter 6, which sug- gests that aspects of meaning can be learned solely from the texts we encounter over our lives, based on the complex association of words with the words they co-occur with (and with the words that those words occur with).'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.048411497730711045
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Such facts have motivated the distributional hypothesis of Chapter 6, which sug-
gests that aspects of meaning can be learned solely from the texts we encounter over
our lives, based on the complex association of words with the words they co-occur
with (and with the words that those words occur with).', Similarity: 0.048411497730711045
Comparing: 
  Sentence: 'the distributional hypothe- sis suggests both that we can acquire remarkable amounts of knowledge from text, and that this knowledge can be brought to bear long after its initial acquisition.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.01090909090909091
Comparing: 
  Sentence: 'of course, grounding from real-world interaction or other modalities can help build even more powerful models, but even text alone is remarkably useful.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.03913894324853229
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Of
course, grounding from real-world interaction or other modalities can help build
even more powerful models, but even text alone is remarkably useful.', Similarity: 0.03913894324853229
Comparing: 
  Sentence: 'in this chapter we formalize this idea of pretraining—learning knowledge about pretraining language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.014010507880910683
Comparing: 
  Sentence: 'large language models exhibit remark-'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.18181818181818182
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Large language models exhibit remark-', Similarity: 0.18181818181818182
Comparing: 
  Sentence: 'speech and language processing.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.05194805194805195
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Speech and Language Processing.', Similarity: 0.05194805194805195
Comparing: 
  Sentence: 'daniel jurafsky & james h.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.017699115044247787
Comparing: 
  Sentence: 'martin.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.00966183574879227
Comparing: 
  Sentence: 'copyright © 2024.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.027649769585253458
Comparing: 
  Sentence: 'all rights reserved.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.01818181818181818
Comparing: 
  Sentence: 'draft of august 20, 2024.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.008888888888888889
Comparing: 
  Sentence: 'chapter 10 large language models “how much do we know at any time?'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.07518796992481203
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'CHAPTER
10
Large Language Models
“How much do we know at any time?', Similarity: 0.07518796992481203
Comparing: 
  Sentence: 'much more, or so i believe, than we know we know.” agatha christie, the moving finger fluent speakers of a language bring an enormous amount of knowledge to bear dur- ing comprehension and production.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.025
Comparing: 
  Sentence: 'this knowledge is embodied in many forms, perhaps most obviously in the vocabulary, the rich representations we have of words and their meanings and usage.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.09014084507042254
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'This knowledge is embodied in many forms,
perhaps most obviously in the vocabulary, the rich representations we have of words
and their meanings and usage.', Similarity: 0.09014084507042254
Comparing: 
  Sentence: 'this makes the vocabulary a useful lens to explore the acquisition of knowledge from text, by both people and machines.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.050156739811912224
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'This makes the vocabulary a useful lens to explore
the acquisition of knowledge from text, by both people and machines.', Similarity: 0.050156739811912224
Comparing: 
  Sentence: 'estimates of the size of adult vocabularies vary widely both within and across languages.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.06920415224913495
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Estimates of the size of adult vocabularies vary widely both within and across
languages.', Similarity: 0.06920415224913495
Comparing: 
  Sentence: 'for example, estimates of the vocabulary size of young adult speakers of american english range from 30,000 to 100,000 depending on the resources used to make the estimate and the deﬁnition of what it means to know a word.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.018957345971563982
Comparing: 
  Sentence: 'what is agreed upon is that the vast majority of words that mature speakers use in their day-to-day interactions are acquired early in life through spoken interactions with caregivers and peers, usually well before the start of formal schooling.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.0449438202247191
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'What
is agreed upon is that the vast majority of words that mature speakers use in their
day-to-day interactions are acquired early in life through spoken interactions with
caregivers and peers, usually well before the start of formal schooling.', Similarity: 0.0449438202247191
Comparing: 
  Sentence: 'this active vocabulary (usually on the order of 2000 words for young speakers) is extremely limited compared to the size of the adult vocabulary, and is quite stable, with very few additional words learned via casual conversation beyond this early stage.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.02643171806167401
Comparing: 
  Sentence: 'obvi- ously, this leaves a very large number of words to be acquired by other means.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.035211267605633804
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Obvi-
ously, this leaves a very large number of words to be acquired by other means.', Similarity: 0.035211267605633804
Comparing: 
  Sentence: 'a simple consequence of these facts is that children have to learn about 7 to 10 words a day, every single day, to arrive at observed vocabulary levels by the time they are 20 years of age.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.02056555269922879
Comparing: 
  Sentence: 'and indeed empirical estimates of vocabulary growth in late el- ementary through high school are consistent with this rate.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.030959752321981424
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'And indeed empirical estimates of vocabulary growth in late el-
ementary through high school are consistent with this rate.', Similarity: 0.030959752321981424
Comparing: 
  Sentence: 'how do children achieve this rate of vocabulary growth?'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.023529411764705882
Comparing: 
  Sentence: 'the bulk of this knowledge acquisition seems to happen as a by-product of reading, as part of the rich processing and reasoning that we perform when we read.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.028011204481792718
Comparing: 
  Sentence: 'research into the average amount of time children spend reading, and the lexical diversity of the texts they read, indicate that it is possible to achieve the desired rate.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.03763440860215054
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Research into the average amount of time children spend
reading, and the lexical diversity of the texts they read, indicate that it is possible
to achieve the desired rate.', Similarity: 0.03763440860215054
Comparing: 
  Sentence: 'but the mechanism behind this rate of learning must be remarkable indeed, since at some points during learning the rate of vocabulary growth exceeds the rate at which new words are appearing to the learner!'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.019704433497536946
Comparing: 
  Sentence: 'such facts have motivated the distributional hypothesis of chapter 6, which sug- gests that aspects of meaning can be learned solely from the texts we encounter over our lives, based on the complex association of words with the words they co-occur with (and with the words that those words occur with).'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.0199203187250996
Comparing: 
  Sentence: 'the distributional hypothe- sis suggests both that we can acquire remarkable amounts of knowledge from text, and that this knowledge can be brought to bear long after its initial acquisition.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.015345268542199489
Comparing: 
  Sentence: 'of course, grounding from real-world interaction or other modalities can help build even more powerful models, but even text alone is remarkably useful.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.03409090909090909
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Of
course, grounding from real-world interaction or other modalities can help build
even more powerful models, but even text alone is remarkably useful.', Similarity: 0.03409090909090909
Comparing: 
  Sentence: 'in this chapter we formalize this idea of pretraining—learning knowledge about pretraining language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.6213592233009708
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'In this chapter we formalize this idea of pretraining—learning knowledge about
pretraining
language and the world from vast amounts of text—and call the resulting pretrained
language models large language models.', Similarity: 0.6213592233009708
Comparing: 
  Sentence: 'large language models exhibit remark-'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.016877637130801686

Highlight Summary:
Total Highlights: 34
Highlighted: True
INFO:     182.66.219.18:25593 - "POST /query-pdf/ HTTP/1.1" 200 OK
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
INFO:     165.227.38.182:38168 - "GET / HTTP/1.1" 404 Not Found
INFO:     165.227.38.182:38180 - "GET /login HTTP/1.1" 404 Not Found
INFO:     154.213.184.18:40088 - "CONNECT example.com%3A443 HTTP/1.1" 404 Not Found
INFO:     60.191.125.35:42238 - "HEAD http%3A//112.124.42.80%3A63435/ HTTP/1.1" 404 Not Found
INFO:     59.126.63.48:46750 - "GET / HTTP/1.0" 404 Not Found
INFO:     49.207.220.184:28661 - "GET /pdf-info HTTP/1.1" 307 Temporary Redirect
INFO:     49.207.220.184:28661 - "GET /pdf-info/ HTTP/1.1" 200 OK
INFO:     49.207.220.184:28661 - "GET /favicon.ico HTTP/1.1" 404 Not Found
INFO:     49.207.220.184:28704 - "OPTIONS /query-pdf/ HTTP/1.1" 200 OK

--- Sentence Matching Process ---
Target Page: 2
Search Phrases: ['LLM', 'LLM', 'LLM', 'Large Language Models', 'Large language models exhibit remarkable performance on all sorts of natural language tasks...', '...causal LM generation', 'Large Language Model']
Similarity Threshold: 0.03

Exact match found for phrase: 'LLM'
Exact match found for phrase: 'LLM'
Exact match found for phrase: 'LLM'
Exact match found for phrase: 'Large Language Models'
Exact match found for phrase: 'Large Language Model'

Highlight Summary:
Total Highlights: 36
Highlighted: True
INFO:     49.207.220.184:28707 - "POST /query-pdf/ HTTP/1.1" 200 OK
INFO:     171.76.85.255:8383 - "OPTIONS /query-pdf/ HTTP/1.1" 200 OK

--- Sentence Matching Process ---
Target Page: 1
Search Phrases: ['In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.', 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.']
Similarity Threshold: 0.03

Comparing: 
  Sentence: 'speech and language processing.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.01839080459770115
Comparing: 
  Sentence: 'daniel jurafsky & james h.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.018604651162790697
Comparing: 
  Sentence: 'martin.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.004866180048661801
Comparing: 
  Sentence: 'copyright © 2024.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.014251781472684086
Comparing: 
  Sentence: 'all rights reserved.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.009433962264150943
Comparing: 
  Sentence: 'draft of august 20, 2024.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.013986013986013986
Comparing: 
  Sentence: 'chapter 10 large language models “how much do we know at any time?'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.05106382978723404
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.', Sentence: 'CHAPTER
10
Large Language Models
“How much do we know at any time?', Similarity: 0.05106382978723404
Comparing: 
  Sentence: 'much more, or so i believe, than we know we know.” agatha christie, the moving finger fluent speakers of a language bring an enormous amount of knowledge to bear dur- ing comprehension and production.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.026490066225165563
Comparing: 
  Sentence: 'this knowledge is embodied in many forms, perhaps most obviously in the vocabulary, the rich representations we have of words and their meanings and usage.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.06797853309481217
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.', Sentence: 'This knowledge is embodied in many forms,
perhaps most obviously in the vocabulary, the rich representations we have of words
and their meanings and usage.', Similarity: 0.06797853309481217
Comparing: 
  Sentence: 'this makes the vocabulary a useful lens to explore the acquisition of knowledge from text, by both people and machines.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.03824091778202677
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.', Sentence: 'This makes the vocabulary a useful lens to explore
the acquisition of knowledge from text, by both people and machines.', Similarity: 0.03824091778202677
Comparing: 
  Sentence: 'estimates of the size of adult vocabularies vary widely both within and across languages.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.07302231237322515
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.', Sentence: 'Estimates of the size of adult vocabularies vary widely both within and across
languages.', Similarity: 0.07302231237322515
Comparing: 
  Sentence: 'for example, estimates of the vocabulary size of young adult speakers of american english range from 30,000 to 100,000 depending on the resources used to make the estimate and the deﬁnition of what it means to know a word.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.022364217252396165
Comparing: 
  Sentence: 'what is agreed upon is that the vast majority of words that mature speakers use in their day-to-day interactions are acquired early in life through spoken interactions with caregivers and peers, usually well before the start of formal schooling.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.040061633281972264
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.', Sentence: 'What
is agreed upon is that the vast majority of words that mature speakers use in their
day-to-day interactions are acquired early in life through spoken interactions with
caregivers and peers, usually well before the start of formal schooling.', Similarity: 0.040061633281972264
Comparing: 
  Sentence: 'this active vocabulary (usually on the order of 2000 words for young speakers) is extremely limited compared to the size of the adult vocabulary, and is quite stable, with very few additional words learned via casual conversation beyond this early stage.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.03951367781155015
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.', Sentence: 'This active
vocabulary (usually on the order of 2000 words for young speakers) is extremely
limited compared to the size of the adult vocabulary, and is quite stable, with very
few additional words learned via casual conversation beyond this early stage.', Similarity: 0.03951367781155015
Comparing: 
  Sentence: 'obvi- ously, this leaves a very large number of words to be acquired by other means.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.036885245901639344
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.', Sentence: 'Obvi-
ously, this leaves a very large number of words to be acquired by other means.', Similarity: 0.036885245901639344
Comparing: 
  Sentence: 'a simple consequence of these facts is that children have to learn about 7 to 10 words a day, every single day, to arrive at observed vocabulary levels by the time they are 20 years of age.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.026981450252951095
Comparing: 
  Sentence: 'and indeed empirical estimates of vocabulary growth in late el- ementary through high school are consistent with this rate.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.04554079696394687
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.', Sentence: 'And indeed empirical estimates of vocabulary growth in late el-
ementary through high school are consistent with this rate.', Similarity: 0.04554079696394687
Comparing: 
  Sentence: 'how do children achieve this rate of vocabulary growth?'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.030501089324618737
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.', Sentence: 'How do children achieve
this rate of vocabulary growth?', Similarity: 0.030501089324618737
Comparing: 
  Sentence: 'the bulk of this knowledge acquisition seems to happen as a by-product of reading, as part of the rich processing and reasoning that we perform when we read.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.053475935828877004
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.', Sentence: 'The bulk of this knowledge acquisition seems to
happen as a by-product of reading, as part of the rich processing and reasoning that
we perform when we read.', Similarity: 0.053475935828877004
Comparing: 
  Sentence: 'research into the average amount of time children spend reading, and the lexical diversity of the texts they read, indicate that it is possible to achieve the desired rate.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.0798611111111111
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.', Sentence: 'Research into the average amount of time children spend
reading, and the lexical diversity of the texts they read, indicate that it is possible
to achieve the desired rate.', Similarity: 0.0798611111111111
Comparing: 
  Sentence: 'but the mechanism behind this rate of learning must be remarkable indeed, since at some points during learning the rate of vocabulary growth exceeds the rate at which new words are appearing to the learner!'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.029508196721311476
Comparing: 
  Sentence: 'such facts have motivated the distributional hypothesis of chapter 6, which sug- gests that aspects of meaning can be learned solely from the texts we encounter over our lives, based on the complex association of words with the words they co-occur with (and with the words that those words occur with).'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.0339943342776204
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.', Sentence: 'Such facts have motivated the distributional hypothesis of Chapter 6, which sug-
gests that aspects of meaning can be learned solely from the texts we encounter over
our lives, based on the complex association of words with the words they co-occur
with (and with the words that those words occur with).', Similarity: 0.0339943342776204
Comparing: 
  Sentence: 'the distributional hypothe- sis suggests both that we can acquire remarkable amounts of knowledge from text, and that this knowledge can be brought to bear long after its initial acquisition.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.02689075630252101
Comparing: 
  Sentence: 'of course, grounding from real-world interaction or other modalities can help build even more powerful models, but even text alone is remarkably useful.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.03237410071942446
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.', Sentence: 'Of
course, grounding from real-world interaction or other modalities can help build
even more powerful models, but even text alone is remarkably useful.', Similarity: 0.03237410071942446
Comparing: 
  Sentence: 'in this chapter we formalize this idea of pretraining—learning knowledge about pretraining language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.6493506493506493
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.', Sentence: 'In this chapter we formalize this idea of pretraining—learning knowledge about
pretraining
language and the world from vast amounts of text—and call the resulting pretrained
language models large language models.', Similarity: 0.6493506493506493
Comparing: 
  Sentence: 'large language models exhibit remark-'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.'
  Similarity: 0.06349206349206349
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book.', Sentence: 'Large language models exhibit remark-', Similarity: 0.06349206349206349
Comparing: 
  Sentence: 'speech and language processing.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.22580645161290322
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Speech and Language Processing.', Similarity: 0.22580645161290322
Comparing: 
  Sentence: 'daniel jurafsky & james h.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.17679558011049723
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Daniel Jurafsky & James H.', Similarity: 0.17679558011049723
Comparing: 
  Sentence: 'martin.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.08641975308641975
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Martin.', Similarity: 0.08641975308641975
Comparing: 
  Sentence: 'copyright © 2024.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.10465116279069768
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Copyright © 2024.', Similarity: 0.10465116279069768
Comparing: 
  Sentence: 'all rights reserved.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.12571428571428572
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'All
rights reserved.', Similarity: 0.12571428571428572
Comparing: 
  Sentence: 'draft of august 20, 2024.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.1111111111111111
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Draft of August 20, 2024.', Similarity: 0.1111111111111111
Comparing: 
  Sentence: 'chapter 10 large language models “how much do we know at any time?'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.18099547511312217
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'CHAPTER
10
Large Language Models
“How much do we know at any time?', Similarity: 0.18099547511312217
Comparing: 
  Sentence: 'much more, or so i believe, than we know we know.” agatha christie, the moving finger fluent speakers of a language bring an enormous amount of knowledge to bear dur- ing comprehension and production.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.2028169014084507
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Much more, or so I believe, than we
know we know.”
Agatha Christie, The Moving Finger
Fluent speakers of a language bring an enormous amount of knowledge to bear dur-
ing comprehension and production.', Similarity: 0.2028169014084507
Comparing: 
  Sentence: 'this knowledge is embodied in many forms, perhaps most obviously in the vocabulary, the rich representations we have of words and their meanings and usage.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.18064516129032257
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'This knowledge is embodied in many forms,
perhaps most obviously in the vocabulary, the rich representations we have of words
and their meanings and usage.', Similarity: 0.18064516129032257
Comparing: 
  Sentence: 'this makes the vocabulary a useful lens to explore the acquisition of knowledge from text, by both people and machines.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.291970802919708
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'This makes the vocabulary a useful lens to explore
the acquisition of knowledge from text, by both people and machines.', Similarity: 0.291970802919708
Comparing: 
  Sentence: 'estimates of the size of adult vocabularies vary widely both within and across languages.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.12295081967213115
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Estimates of the size of adult vocabularies vary widely both within and across
languages.', Similarity: 0.12295081967213115
Comparing: 
  Sentence: 'for example, estimates of the vocabulary size of young adult speakers of american english range from 30,000 to 100,000 depending on the resources used to make the estimate and the deﬁnition of what it means to know a word.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.23342175066312998
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'For example, estimates of the vocabulary size of young adult speakers of
American English range from 30,000 to 100,000 depending on the resources used
to make the estimate and the deﬁnition of what it means to know a word.', Similarity: 0.23342175066312998
Comparing: 
  Sentence: 'what is agreed upon is that the vast majority of words that mature speakers use in their day-to-day interactions are acquired early in life through spoken interactions with caregivers and peers, usually well before the start of formal schooling.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.18
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'What
is agreed upon is that the vast majority of words that mature speakers use in their
day-to-day interactions are acquired early in life through spoken interactions with
caregivers and peers, usually well before the start of formal schooling.', Similarity: 0.18
Comparing: 
  Sentence: 'this active vocabulary (usually on the order of 2000 words for young speakers) is extremely limited compared to the size of the adult vocabulary, and is quite stable, with very few additional words learned via casual conversation beyond this early stage.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.23471882640586797
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'This active
vocabulary (usually on the order of 2000 words for young speakers) is extremely
limited compared to the size of the adult vocabulary, and is quite stable, with very
few additional words learned via casual conversation beyond this early stage.', Similarity: 0.23471882640586797
Comparing: 
  Sentence: 'obvi- ously, this leaves a very large number of words to be acquired by other means.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.2510460251046025
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Obvi-
ously, this leaves a very large number of words to be acquired by other means.', Similarity: 0.2510460251046025
Comparing: 
  Sentence: 'a simple consequence of these facts is that children have to learn about 7 to 10 words a day, every single day, to arrive at observed vocabulary levels by the time they are 20 years of age.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.23255813953488372
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'A simple consequence of these facts is that children have to learn about 7 to 10
words a day, every single day, to arrive at observed vocabulary levels by the time they
are 20 years of age.', Similarity: 0.23255813953488372
Comparing: 
  Sentence: 'and indeed empirical estimates of vocabulary growth in late el- ementary through high school are consistent with this rate.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.17266187050359713
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'And indeed empirical estimates of vocabulary growth in late el-
ementary through high school are consistent with this rate.', Similarity: 0.17266187050359713
Comparing: 
  Sentence: 'how do children achieve this rate of vocabulary growth?'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.20952380952380953
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'How do children achieve
this rate of vocabulary growth?', Similarity: 0.20952380952380953
Comparing: 
  Sentence: 'the bulk of this knowledge acquisition seems to happen as a by-product of reading, as part of the rich processing and reasoning that we perform when we read.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.27564102564102566
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'The bulk of this knowledge acquisition seems to
happen as a by-product of reading, as part of the rich processing and reasoning that
we perform when we read.', Similarity: 0.27564102564102566
Comparing: 
  Sentence: 'research into the average amount of time children spend reading, and the lexical diversity of the texts they read, indicate that it is possible to achieve the desired rate.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.25688073394495414
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Research into the average amount of time children spend
reading, and the lexical diversity of the texts they read, indicate that it is possible
to achieve the desired rate.', Similarity: 0.25688073394495414
Comparing: 
  Sentence: 'but the mechanism behind this rate of learning must be remarkable indeed, since at some points during learning the rate of vocabulary growth exceeds the rate at which new words are appearing to the learner!'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.21606648199445982
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'But the mechanism behind this rate of learning must
be remarkable indeed, since at some points during learning the rate of vocabulary
growth exceeds the rate at which new words are appearing to the learner!', Similarity: 0.21606648199445982
Comparing: 
  Sentence: 'such facts have motivated the distributional hypothesis of chapter 6, which sug- gests that aspects of meaning can be learned solely from the texts we encounter over our lives, based on the complex association of words with the words they co-occur with (and with the words that those words occur with).'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.2188183807439825
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Such facts have motivated the distributional hypothesis of Chapter 6, which sug-
gests that aspects of meaning can be learned solely from the texts we encounter over
our lives, based on the complex association of words with the words they co-occur
with (and with the words that those words occur with).', Similarity: 0.2188183807439825
Comparing: 
  Sentence: 'the distributional hypothe- sis suggests both that we can acquire remarkable amounts of knowledge from text, and that this knowledge can be brought to bear long after its initial acquisition.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.2658959537572254
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'The distributional hypothe-
sis suggests both that we can acquire remarkable amounts of knowledge from text,
and that this knowledge can be brought to bear long after its initial acquisition.', Similarity: 0.2658959537572254
Comparing: 
  Sentence: 'of course, grounding from real-world interaction or other modalities can help build even more powerful models, but even text alone is remarkably useful.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.18241042345276873
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Of
course, grounding from real-world interaction or other modalities can help build
even more powerful models, but even text alone is remarkably useful.', Similarity: 0.18241042345276873
Comparing: 
  Sentence: 'in this chapter we formalize this idea of pretraining—learning knowledge about pretraining language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.20708446866485014
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'In this chapter we formalize this idea of pretraining—learning knowledge about
pretraining
language and the world from vast amounts of text—and call the resulting pretrained
language models large language models.', Similarity: 0.20708446866485014
Comparing: 
  Sentence: 'large language models exhibit remark-'
  Target:   'they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.20833333333333334
Fuzzy match found! Phrase: 'They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Large language models exhibit remark-', Similarity: 0.20833333333333334

Highlight Summary:
Total Highlights: 88
Highlighted: True
INFO:     171.76.85.255:6625 - "POST /query-pdf/ HTTP/1.1" 200 OK
INFO:     171.76.85.255:1573 - "OPTIONS /query-pdf/ HTTP/1.1" 200 OK

--- Sentence Matching Process ---
Target Page: 5
Search Phrases: ["Recall from Chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. To generate text from a trained transformer language model we'll just generalize this model a bit: at each step we'll sample words according to their probability conditioned on our previous choices, and we'll use a transformer language model as the probability model that tells us this probability.", 'The problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.']
Similarity Threshold: 0.03

Comparing: 
  Sentence: '10.2 • sampling for llm generation 5 original story generated summary … idea kyle was born.'
  Target:   'recall from chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. to generate text from a trained transformer language model we'll just generalize this model a bit: at each step we'll sample words according to their probability conditioned on our previous choices, and we'll use a transformer language model as the probability model that tells us this probability.'
  Similarity: 0.012759170653907496
Comparing: 
  Sentence: 'kyle waring waring only the … will delimiter will u u u tl;dr lm head e e e e e e e e … figure 10.3 summarization with large language models using the tl;dr token and context-based autore- gressive generation.'
  Target:   'recall from chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. to generate text from a trained transformer language model we'll just generalize this model a bit: at each step we'll sample words according to their probability conditioned on our previous choices, and we'll use a transformer language model as the probability model that tells us this probability.'
  Similarity: 0.010738255033557046
Comparing: 
  Sentence: 'is identical, and the probabilistic model is the same, greedy decoding will always re- sult in generating exactly the same string.'
  Target:   'recall from chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. to generate text from a trained transformer language model we'll just generalize this model a bit: at each step we'll sample words according to their probability conditioned on our previous choices, and we'll use a transformer language model as the probability model that tells us this probability.'
  Similarity: 0.015015015015015015
Comparing: 
  Sentence: 'we’ll see in chapter 13 that an extension to greedy decoding called beam search works well in tasks like machine translation, which are very constrained in that we are always generating a text in one language conditioned on a very speciﬁc text in another language.'
  Target:   'recall from chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. to generate text from a trained transformer language model we'll just generalize this model a bit: at each step we'll sample words according to their probability conditioned on our previous choices, and we'll use a transformer language model as the probability model that tells us this probability.'
  Similarity: 0.0225
Comparing: 
  Sentence: 'in most other tasks, how- ever, people prefer text which has been generated by more sophisticated methods, called sampling methods, that introduce a bit more diversity into the generations.'
  Target:   'recall from chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. to generate text from a trained transformer language model we'll just generalize this model a bit: at each step we'll sample words according to their probability conditioned on our previous choices, and we'll use a transformer language model as the probability model that tells us this probability.'
  Similarity: 0.016551724137931035
Comparing: 
  Sentence: 'we’ll see how to do that in the next few sections.'
  Target:   'recall from chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. to generate text from a trained transformer language model we'll just generalize this model a bit: at each step we'll sample words according to their probability conditioned on our previous choices, and we'll use a transformer language model as the probability model that tells us this probability.'
  Similarity: 0.020477815699658702
Comparing: 
  Sentence: '10.2 sampling for llm generation the core of the generation process for large language models is the task of choosing the single word to generate next based on the context and based on the probabilities that the model assigns to possible words.'
  Target:   'recall from chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. to generate text from a trained transformer language model we'll just generalize this model a bit: at each step we'll sample words according to their probability conditioned on our previous choices, and we'll use a transformer language model as the probability model that tells us this probability.'
  Similarity: 0.020512820512820513
Comparing: 
  Sentence: 'this task of choosing a word to generate based on the model’s probabilities is called decoding.'
  Target:   'recall from chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. to generate text from a trained transformer language model we'll just generalize this model a bit: at each step we'll sample words according to their probability conditioned on our previous choices, and we'll use a transformer language model as the probability model that tells us this probability.'
  Similarity: 0.009508716323296355
Comparing: 
  Sentence: 'decoding from a language decoding model in a left-to-right manner (or right-to-left for languages like arabic in which we read from right to left), and thus repeatedly choosing the next word conditioned on our previous choices is called autoregressive generation or causal lm genera- autoregressive generation tion.1 (as we’ll see, alternatives like the masked language models of chapter 11 are non-causal because they can predict words based on both past and future words).'
  Target:   'recall from chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. to generate text from a trained transformer language model we'll just generalize this model a bit: at each step we'll sample words according to their probability conditioned on our previous choices, and we'll use a transformer language model as the probability model that tells us this probability.'
  Similarity: 0.031683168316831684
Fuzzy match found! Phrase: 'Recall from Chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. To generate text from a trained transformer language model we'll just generalize this model a bit: at each step we'll sample words according to their probability conditioned on our previous choices, and we'll use a transformer language model as the probability model that tells us this probability.', Sentence: 'Decoding from a language
decoding
model in a left-to-right manner (or right-to-left for languages like Arabic in which
we read from right to left), and thus repeatedly choosing the next word conditioned
on our previous choices is called autoregressive generation or causal LM genera-
autoregressive
generation
tion.1 (As we’ll see, alternatives like the masked language models of Chapter 11 are
non-causal because they can predict words based on both past and future words).', Similarity: 0.031683168316831684
Comparing: 
  Sentence: 'the most common method for decoding in large language models is sampling.'
  Target:   'recall from chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. to generate text from a trained transformer language model we'll just generalize this model a bit: at each step we'll sample words according to their probability conditioned on our previous choices, and we'll use a transformer language model as the probability model that tells us this probability.'
  Similarity: 0.009852216748768473
Comparing: 
  Sentence: 'recall from chapter 3 that sampling from a model’s distribution over words means sampling to choose random words according to their probability assigned by the model.'
  Target:   'recall from chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. to generate text from a trained transformer language model we'll just generalize this model a bit: at each step we'll sample words according to their probability conditioned on our previous choices, and we'll use a transformer language model as the probability model that tells us this probability.'
  Similarity: 0.09116809116809117
Fuzzy match found! Phrase: 'Recall from Chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. To generate text from a trained transformer language model we'll just generalize this model a bit: at each step we'll sample words according to their probability conditioned on our previous choices, and we'll use a transformer language model as the probability model that tells us this probability.', Sentence: 'Recall from Chapter 3 that sampling from a model’s distribution over words means
sampling
to choose random words according to their probability assigned by the model.', Similarity: 0.09116809116809117
Comparing: 
  Sentence: 'that is, we iteratively choose a word to generate according to its probability in context 1 technically an autoregressive model predicts a value at time t based on a linear function of the values at times t −1, t −2, and so on.'
  Target:   'recall from chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. to generate text from a trained transformer language model we'll just generalize this model a bit: at each step we'll sample words according to their probability conditioned on our previous choices, and we'll use a transformer language model as the probability model that tells us this probability.'
  Similarity: 0.01310615989515072
Comparing: 
  Sentence: 'although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive since the word generated at each time step is conditioned on the word selected by the network from the previous step.'
  Target:   'recall from chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. to generate text from a trained transformer language model we'll just generalize this model a bit: at each step we'll sample words according to their probability conditioned on our previous choices, and we'll use a transformer language model as the probability model that tells us this probability.'
  Similarity: 0.007434944237918215
Comparing: 
  Sentence: ''
  Target:   'recall from chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. to generate text from a trained transformer language model we'll just generalize this model a bit: at each step we'll sample words according to their probability conditioned on our previous choices, and we'll use a transformer language model as the probability model that tells us this probability.'
  Similarity: 0.0
Comparing: 
  Sentence: '10.2 • sampling for llm generation 5 original story generated summary … idea kyle was born.'
  Target:   'the problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.'
  Similarity: 0.004123711340206186
Comparing: 
  Sentence: 'kyle waring waring only the … will delimiter will u u u tl;dr lm head e e e e e e e e … figure 10.3 summarization with large language models using the tl;dr token and context-based autore- gressive generation.'
  Target:   'the problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.'
  Similarity: 0.006633499170812604
Comparing: 
  Sentence: 'is identical, and the probabilistic model is the same, greedy decoding will always re- sult in generating exactly the same string.'
  Target:   'the problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.'
  Similarity: 0.019083969465648856
Comparing: 
  Sentence: 'we’ll see in chapter 13 that an extension to greedy decoding called beam search works well in tasks like machine translation, which are very constrained in that we are always generating a text in one language conditioned on a very speciﬁc text in another language.'
  Target:   'the problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.'
  Similarity: 0.0243161094224924
Comparing: 
  Sentence: 'in most other tasks, how- ever, people prefer text which has been generated by more sophisticated methods, called sampling methods, that introduce a bit more diversity into the generations.'
  Target:   'the problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.'
  Similarity: 0.03773584905660377
Fuzzy match found! Phrase: 'The problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.', Sentence: 'In most other tasks, how-
ever, people prefer text which has been generated by more sophisticated methods,
called sampling methods, that introduce a bit more diversity into the generations.', Similarity: 0.03773584905660377
Comparing: 
  Sentence: 'we’ll see how to do that in the next few sections.'
  Target:   'the problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.'
  Similarity: 0.018018018018018018
Comparing: 
  Sentence: '10.2 sampling for llm generation the core of the generation process for large language models is the task of choosing the single word to generate next based on the context and based on the probabilities that the model assigns to possible words.'
  Target:   'the problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.'
  Similarity: 0.003134796238244514
Comparing: 
  Sentence: 'this task of choosing a word to generate based on the model’s probabilities is called decoding.'
  Target:   'the problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.'
  Similarity: 0.049079754601226995
Fuzzy match found! Phrase: 'The problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.', Sentence: 'This task of choosing a word to generate
based on the model’s probabilities is called decoding.', Similarity: 0.049079754601226995
Comparing: 
  Sentence: 'decoding from a language decoding model in a left-to-right manner (or right-to-left for languages like arabic in which we read from right to left), and thus repeatedly choosing the next word conditioned on our previous choices is called autoregressive generation or causal lm genera- autoregressive generation tion.1 (as we’ll see, alternatives like the masked language models of chapter 11 are non-causal because they can predict words based on both past and future words).'
  Target:   'the problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.'
  Similarity: 0.020737327188940093
Comparing: 
  Sentence: 'the most common method for decoding in large language models is sampling.'
  Target:   'the problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.'
  Similarity: 0.03854389721627409
Fuzzy match found! Phrase: 'The problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.', Sentence: 'The most common method for decoding in large language models is sampling.', Similarity: 0.03854389721627409
Comparing: 
  Sentence: 'recall from chapter 3 that sampling from a model’s distribution over words means sampling to choose random words according to their probability assigned by the model.'
  Target:   'the problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.'
  Similarity: 0.039285714285714285
Fuzzy match found! Phrase: 'The problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.', Sentence: 'Recall from Chapter 3 that sampling from a model’s distribution over words means
sampling
to choose random words according to their probability assigned by the model.', Similarity: 0.039285714285714285
Comparing: 
  Sentence: 'that is, we iteratively choose a word to generate according to its probability in context 1 technically an autoregressive model predicts a value at time t based on a linear function of the values at times t −1, t −2, and so on.'
  Target:   'the problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.'
  Similarity: 0.0322061191626409
Fuzzy match found! Phrase: 'The problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.', Sentence: 'That
is, we iteratively choose a word to generate according to its probability in context
1
Technically an autoregressive model predicts a value at time t based on a linear function of the values
at times t −1, t −2, and so on.', Similarity: 0.0322061191626409
Comparing: 
  Sentence: 'although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive since the word generated at each time step is conditioned on the word selected by the network from the previous step.'
  Target:   'the problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.'
  Similarity: 0.03007518796992481
Fuzzy match found! Phrase: 'The problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.', Sentence: 'Although language models are not linear (since they have many layers of
non-linearities), we loosely refer to this generation technique as autoregressive since the word generated
at each time step is conditioned on the word selected by the network from the previous step.', Similarity: 0.03007518796992481
Comparing: 
  Sentence: ''
  Target:   'the problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences.'
  Similarity: 0.0

Highlight Summary:
Total Highlights: 25
Highlighted: True
INFO:     171.76.85.255:15596 - "POST /query-pdf/ HTTP/1.1" 200 OK

--- Sentence Matching Process ---
Target Page: 1
Search Phrases: ['In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.']
Similarity Threshold: 0.03

Comparing: 
  Sentence: 'speech and language processing.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.05194805194805195
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Speech and Language Processing.', Similarity: 0.05194805194805195
Comparing: 
  Sentence: 'daniel jurafsky & james h.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.017699115044247787
Comparing: 
  Sentence: 'martin.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.00966183574879227
Comparing: 
  Sentence: 'copyright © 2024.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.027649769585253458
Comparing: 
  Sentence: 'all rights reserved.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.01818181818181818
Comparing: 
  Sentence: 'draft of august 20, 2024.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.008888888888888889
Comparing: 
  Sentence: 'chapter 10 large language models “how much do we know at any time?'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.07518796992481203
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'CHAPTER
10
Large Language Models
“How much do we know at any time?', Similarity: 0.07518796992481203
Comparing: 
  Sentence: 'much more, or so i believe, than we know we know.” agatha christie, the moving finger fluent speakers of a language bring an enormous amount of knowledge to bear dur- ing comprehension and production.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.025
Comparing: 
  Sentence: 'this knowledge is embodied in many forms, perhaps most obviously in the vocabulary, the rich representations we have of words and their meanings and usage.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.09014084507042254
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'This knowledge is embodied in many forms,
perhaps most obviously in the vocabulary, the rich representations we have of words
and their meanings and usage.', Similarity: 0.09014084507042254
Comparing: 
  Sentence: 'this makes the vocabulary a useful lens to explore the acquisition of knowledge from text, by both people and machines.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.050156739811912224
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'This makes the vocabulary a useful lens to explore
the acquisition of knowledge from text, by both people and machines.', Similarity: 0.050156739811912224
Comparing: 
  Sentence: 'estimates of the size of adult vocabularies vary widely both within and across languages.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.06920415224913495
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Estimates of the size of adult vocabularies vary widely both within and across
languages.', Similarity: 0.06920415224913495
Comparing: 
  Sentence: 'for example, estimates of the vocabulary size of young adult speakers of american english range from 30,000 to 100,000 depending on the resources used to make the estimate and the deﬁnition of what it means to know a word.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.018957345971563982
Comparing: 
  Sentence: 'what is agreed upon is that the vast majority of words that mature speakers use in their day-to-day interactions are acquired early in life through spoken interactions with caregivers and peers, usually well before the start of formal schooling.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.0449438202247191
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'What
is agreed upon is that the vast majority of words that mature speakers use in their
day-to-day interactions are acquired early in life through spoken interactions with
caregivers and peers, usually well before the start of formal schooling.', Similarity: 0.0449438202247191
Comparing: 
  Sentence: 'this active vocabulary (usually on the order of 2000 words for young speakers) is extremely limited compared to the size of the adult vocabulary, and is quite stable, with very few additional words learned via casual conversation beyond this early stage.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.02643171806167401
Comparing: 
  Sentence: 'obvi- ously, this leaves a very large number of words to be acquired by other means.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.035211267605633804
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Obvi-
ously, this leaves a very large number of words to be acquired by other means.', Similarity: 0.035211267605633804
Comparing: 
  Sentence: 'a simple consequence of these facts is that children have to learn about 7 to 10 words a day, every single day, to arrive at observed vocabulary levels by the time they are 20 years of age.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.02056555269922879
Comparing: 
  Sentence: 'and indeed empirical estimates of vocabulary growth in late el- ementary through high school are consistent with this rate.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.030959752321981424
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'And indeed empirical estimates of vocabulary growth in late el-
ementary through high school are consistent with this rate.', Similarity: 0.030959752321981424
Comparing: 
  Sentence: 'how do children achieve this rate of vocabulary growth?'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.023529411764705882
Comparing: 
  Sentence: 'the bulk of this knowledge acquisition seems to happen as a by-product of reading, as part of the rich processing and reasoning that we perform when we read.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.028011204481792718
Comparing: 
  Sentence: 'research into the average amount of time children spend reading, and the lexical diversity of the texts they read, indicate that it is possible to achieve the desired rate.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.03763440860215054
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Research into the average amount of time children spend
reading, and the lexical diversity of the texts they read, indicate that it is possible
to achieve the desired rate.', Similarity: 0.03763440860215054
Comparing: 
  Sentence: 'but the mechanism behind this rate of learning must be remarkable indeed, since at some points during learning the rate of vocabulary growth exceeds the rate at which new words are appearing to the learner!'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.019704433497536946
Comparing: 
  Sentence: 'such facts have motivated the distributional hypothesis of chapter 6, which sug- gests that aspects of meaning can be learned solely from the texts we encounter over our lives, based on the complex association of words with the words they co-occur with (and with the words that those words occur with).'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.0199203187250996
Comparing: 
  Sentence: 'the distributional hypothe- sis suggests both that we can acquire remarkable amounts of knowledge from text, and that this knowledge can be brought to bear long after its initial acquisition.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.015345268542199489
Comparing: 
  Sentence: 'of course, grounding from real-world interaction or other modalities can help build even more powerful models, but even text alone is remarkably useful.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.03409090909090909
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Of
course, grounding from real-world interaction or other modalities can help build
even more powerful models, but even text alone is remarkably useful.', Similarity: 0.03409090909090909
Comparing: 
  Sentence: 'in this chapter we formalize this idea of pretraining—learning knowledge about pretraining language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.6213592233009708
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'In this chapter we formalize this idea of pretraining—learning knowledge about
pretraining
language and the world from vast amounts of text—and call the resulting pretrained
language models large language models.', Similarity: 0.6213592233009708
Comparing: 
  Sentence: 'large language models exhibit remark-'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.016877637130801686
Comparing: 
  Sentence: 'speech and language processing.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.005128205128205128
Comparing: 
  Sentence: 'daniel jurafsky & james h.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.005194805194805195
Comparing: 
  Sentence: 'martin.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.00546448087431694
Comparing: 
  Sentence: 'copyright © 2024.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.005319148936170213
Comparing: 
  Sentence: 'all rights reserved.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.0158311345646438
Comparing: 
  Sentence: 'draft of august 20, 2024.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.005208333333333333
Comparing: 
  Sentence: 'chapter 10 large language models “how much do we know at any time?'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.0
Comparing: 
  Sentence: 'much more, or so i believe, than we know we know.” agatha christie, the moving finger fluent speakers of a language bring an enormous amount of knowledge to bear dur- ing comprehension and production.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.01073345259391771
Comparing: 
  Sentence: 'this knowledge is embodied in many forms, perhaps most obviously in the vocabulary, the rich representations we have of words and their meanings and usage.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.011673151750972763
Comparing: 
  Sentence: 'this makes the vocabulary a useful lens to explore the acquisition of knowledge from text, by both people and machines.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.029288702928870293
Comparing: 
  Sentence: 'estimates of the size of adult vocabularies vary widely both within and across languages.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.017857142857142856
Comparing: 
  Sentence: 'for example, estimates of the vocabulary size of young adult speakers of american english range from 30,000 to 100,000 depending on the resources used to make the estimate and the deﬁnition of what it means to know a word.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.024096385542168676
Comparing: 
  Sentence: 'what is agreed upon is that the vast majority of words that mature speakers use in their day-to-day interactions are acquired early in life through spoken interactions with caregivers and peers, usually well before the start of formal schooling.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.013245033112582781
Comparing: 
  Sentence: 'this active vocabulary (usually on the order of 2000 words for young speakers) is extremely limited compared to the size of the adult vocabulary, and is quite stable, with very few additional words learned via casual conversation beyond this early stage.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.04241435562805873
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'This active
vocabulary (usually on the order of 2000 words for young speakers) is extremely
limited compared to the size of the adult vocabulary, and is quite stable, with very
few additional words learned via casual conversation beyond this early stage.', Similarity: 0.04241435562805873
Comparing: 
  Sentence: 'obvi- ously, this leaves a very large number of words to be acquired by other means.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.03160270880361174
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Obvi-
ously, this leaves a very large number of words to be acquired by other means.', Similarity: 0.03160270880361174
Comparing: 
  Sentence: 'a simple consequence of these facts is that children have to learn about 7 to 10 words a day, every single day, to arrive at observed vocabulary levels by the time they are 20 years of age.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.014598540145985401
Comparing: 
  Sentence: 'and indeed empirical estimates of vocabulary growth in late el- ementary through high school are consistent with this rate.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.008298755186721992
Comparing: 
  Sentence: 'how do children achieve this rate of vocabulary growth?'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.01932367149758454
Comparing: 
  Sentence: 'the bulk of this knowledge acquisition seems to happen as a by-product of reading, as part of the rich processing and reasoning that we perform when we read.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.011627906976744186
Comparing: 
  Sentence: 'research into the average amount of time children spend reading, and the lexical diversity of the texts they read, indicate that it is possible to achieve the desired rate.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.022598870056497175
Comparing: 
  Sentence: 'but the mechanism behind this rate of learning must be remarkable indeed, since at some points during learning the rate of vocabulary growth exceeds the rate at which new words are appearing to the learner!'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.010619469026548672
Comparing: 
  Sentence: 'such facts have motivated the distributional hypothesis of chapter 6, which sug- gests that aspects of meaning can be learned solely from the texts we encounter over our lives, based on the complex association of words with the words they co-occur with (and with the words that those words occur with).'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.048411497730711045
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Such facts have motivated the distributional hypothesis of Chapter 6, which sug-
gests that aspects of meaning can be learned solely from the texts we encounter over
our lives, based on the complex association of words with the words they co-occur
with (and with the words that those words occur with).', Similarity: 0.048411497730711045
Comparing: 
  Sentence: 'the distributional hypothe- sis suggests both that we can acquire remarkable amounts of knowledge from text, and that this knowledge can be brought to bear long after its initial acquisition.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.01090909090909091
Comparing: 
  Sentence: 'of course, grounding from real-world interaction or other modalities can help build even more powerful models, but even text alone is remarkably useful.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.03913894324853229
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Of
course, grounding from real-world interaction or other modalities can help build
even more powerful models, but even text alone is remarkably useful.', Similarity: 0.03913894324853229
Comparing: 
  Sentence: 'in this chapter we formalize this idea of pretraining—learning knowledge about pretraining language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.014010507880910683
Comparing: 
  Sentence: 'large language models exhibit remark-'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.18181818181818182
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Large language models exhibit remark-', Similarity: 0.18181818181818182

Highlight Summary:
Total Highlights: 34
Highlighted: True
INFO:     171.76.85.255:22760 - "POST /query-pdf/ HTTP/1.1" 200 OK
INFO:     49.207.220.184:29491 - "OPTIONS /query-pdf/ HTTP/1.1" 200 OK

--- Sentence Matching Process ---
Target Page: 1
Search Phrases: ['In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.']
Similarity Threshold: 0.03

Comparing: 
  Sentence: 'speech and language processing.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.05194805194805195
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Speech and Language Processing.', Similarity: 0.05194805194805195
Comparing: 
  Sentence: 'daniel jurafsky & james h.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.017699115044247787
Comparing: 
  Sentence: 'martin.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.00966183574879227
Comparing: 
  Sentence: 'copyright © 2024.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.027649769585253458
Comparing: 
  Sentence: 'all rights reserved.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.01818181818181818
Comparing: 
  Sentence: 'draft of august 20, 2024.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.008888888888888889
Comparing: 
  Sentence: 'chapter 10 large language models “how much do we know at any time?'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.07518796992481203
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'CHAPTER
10
Large Language Models
“How much do we know at any time?', Similarity: 0.07518796992481203
Comparing: 
  Sentence: 'much more, or so i believe, than we know we know.” agatha christie, the moving finger fluent speakers of a language bring an enormous amount of knowledge to bear dur- ing comprehension and production.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.025
Comparing: 
  Sentence: 'this knowledge is embodied in many forms, perhaps most obviously in the vocabulary, the rich representations we have of words and their meanings and usage.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.09014084507042254
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'This knowledge is embodied in many forms,
perhaps most obviously in the vocabulary, the rich representations we have of words
and their meanings and usage.', Similarity: 0.09014084507042254
Comparing: 
  Sentence: 'this makes the vocabulary a useful lens to explore the acquisition of knowledge from text, by both people and machines.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.050156739811912224
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'This makes the vocabulary a useful lens to explore
the acquisition of knowledge from text, by both people and machines.', Similarity: 0.050156739811912224
Comparing: 
  Sentence: 'estimates of the size of adult vocabularies vary widely both within and across languages.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.06920415224913495
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Estimates of the size of adult vocabularies vary widely both within and across
languages.', Similarity: 0.06920415224913495
Comparing: 
  Sentence: 'for example, estimates of the vocabulary size of young adult speakers of american english range from 30,000 to 100,000 depending on the resources used to make the estimate and the deﬁnition of what it means to know a word.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.018957345971563982
Comparing: 
  Sentence: 'what is agreed upon is that the vast majority of words that mature speakers use in their day-to-day interactions are acquired early in life through spoken interactions with caregivers and peers, usually well before the start of formal schooling.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.0449438202247191
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'What
is agreed upon is that the vast majority of words that mature speakers use in their
day-to-day interactions are acquired early in life through spoken interactions with
caregivers and peers, usually well before the start of formal schooling.', Similarity: 0.0449438202247191
Comparing: 
  Sentence: 'this active vocabulary (usually on the order of 2000 words for young speakers) is extremely limited compared to the size of the adult vocabulary, and is quite stable, with very few additional words learned via casual conversation beyond this early stage.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.02643171806167401
Comparing: 
  Sentence: 'obvi- ously, this leaves a very large number of words to be acquired by other means.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.035211267605633804
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Obvi-
ously, this leaves a very large number of words to be acquired by other means.', Similarity: 0.035211267605633804
Comparing: 
  Sentence: 'a simple consequence of these facts is that children have to learn about 7 to 10 words a day, every single day, to arrive at observed vocabulary levels by the time they are 20 years of age.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.02056555269922879
Comparing: 
  Sentence: 'and indeed empirical estimates of vocabulary growth in late el- ementary through high school are consistent with this rate.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.030959752321981424
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'And indeed empirical estimates of vocabulary growth in late el-
ementary through high school are consistent with this rate.', Similarity: 0.030959752321981424
Comparing: 
  Sentence: 'how do children achieve this rate of vocabulary growth?'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.023529411764705882
Comparing: 
  Sentence: 'the bulk of this knowledge acquisition seems to happen as a by-product of reading, as part of the rich processing and reasoning that we perform when we read.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.028011204481792718
Comparing: 
  Sentence: 'research into the average amount of time children spend reading, and the lexical diversity of the texts they read, indicate that it is possible to achieve the desired rate.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.03763440860215054
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Research into the average amount of time children spend
reading, and the lexical diversity of the texts they read, indicate that it is possible
to achieve the desired rate.', Similarity: 0.03763440860215054
Comparing: 
  Sentence: 'but the mechanism behind this rate of learning must be remarkable indeed, since at some points during learning the rate of vocabulary growth exceeds the rate at which new words are appearing to the learner!'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.019704433497536946
Comparing: 
  Sentence: 'such facts have motivated the distributional hypothesis of chapter 6, which sug- gests that aspects of meaning can be learned solely from the texts we encounter over our lives, based on the complex association of words with the words they co-occur with (and with the words that those words occur with).'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.0199203187250996
Comparing: 
  Sentence: 'the distributional hypothe- sis suggests both that we can acquire remarkable amounts of knowledge from text, and that this knowledge can be brought to bear long after its initial acquisition.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.015345268542199489
Comparing: 
  Sentence: 'of course, grounding from real-world interaction or other modalities can help build even more powerful models, but even text alone is remarkably useful.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.03409090909090909
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Of
course, grounding from real-world interaction or other modalities can help build
even more powerful models, but even text alone is remarkably useful.', Similarity: 0.03409090909090909
Comparing: 
  Sentence: 'in this chapter we formalize this idea of pretraining—learning knowledge about pretraining language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.6213592233009708
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'In this chapter we formalize this idea of pretraining—learning knowledge about
pretraining
language and the world from vast amounts of text—and call the resulting pretrained
language models large language models.', Similarity: 0.6213592233009708
Comparing: 
  Sentence: 'large language models exhibit remark-'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.016877637130801686
Comparing: 
  Sentence: 'speech and language processing.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.005128205128205128
Comparing: 
  Sentence: 'daniel jurafsky & james h.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.005194805194805195
Comparing: 
  Sentence: 'martin.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.00546448087431694
Comparing: 
  Sentence: 'copyright © 2024.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.005319148936170213
Comparing: 
  Sentence: 'all rights reserved.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.0158311345646438
Comparing: 
  Sentence: 'draft of august 20, 2024.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.005208333333333333
Comparing: 
  Sentence: 'chapter 10 large language models “how much do we know at any time?'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.0
Comparing: 
  Sentence: 'much more, or so i believe, than we know we know.” agatha christie, the moving finger fluent speakers of a language bring an enormous amount of knowledge to bear dur- ing comprehension and production.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.01073345259391771
Comparing: 
  Sentence: 'this knowledge is embodied in many forms, perhaps most obviously in the vocabulary, the rich representations we have of words and their meanings and usage.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.011673151750972763
Comparing: 
  Sentence: 'this makes the vocabulary a useful lens to explore the acquisition of knowledge from text, by both people and machines.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.029288702928870293
Comparing: 
  Sentence: 'estimates of the size of adult vocabularies vary widely both within and across languages.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.017857142857142856
Comparing: 
  Sentence: 'for example, estimates of the vocabulary size of young adult speakers of american english range from 30,000 to 100,000 depending on the resources used to make the estimate and the deﬁnition of what it means to know a word.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.024096385542168676
Comparing: 
  Sentence: 'what is agreed upon is that the vast majority of words that mature speakers use in their day-to-day interactions are acquired early in life through spoken interactions with caregivers and peers, usually well before the start of formal schooling.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.013245033112582781
Comparing: 
  Sentence: 'this active vocabulary (usually on the order of 2000 words for young speakers) is extremely limited compared to the size of the adult vocabulary, and is quite stable, with very few additional words learned via casual conversation beyond this early stage.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.04241435562805873
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'This active
vocabulary (usually on the order of 2000 words for young speakers) is extremely
limited compared to the size of the adult vocabulary, and is quite stable, with very
few additional words learned via casual conversation beyond this early stage.', Similarity: 0.04241435562805873
Comparing: 
  Sentence: 'obvi- ously, this leaves a very large number of words to be acquired by other means.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.03160270880361174
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Obvi-
ously, this leaves a very large number of words to be acquired by other means.', Similarity: 0.03160270880361174
Comparing: 
  Sentence: 'a simple consequence of these facts is that children have to learn about 7 to 10 words a day, every single day, to arrive at observed vocabulary levels by the time they are 20 years of age.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.014598540145985401
Comparing: 
  Sentence: 'and indeed empirical estimates of vocabulary growth in late el- ementary through high school are consistent with this rate.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.008298755186721992
Comparing: 
  Sentence: 'how do children achieve this rate of vocabulary growth?'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.01932367149758454
Comparing: 
  Sentence: 'the bulk of this knowledge acquisition seems to happen as a by-product of reading, as part of the rich processing and reasoning that we perform when we read.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.011627906976744186
Comparing: 
  Sentence: 'research into the average amount of time children spend reading, and the lexical diversity of the texts they read, indicate that it is possible to achieve the desired rate.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.022598870056497175
Comparing: 
  Sentence: 'but the mechanism behind this rate of learning must be remarkable indeed, since at some points during learning the rate of vocabulary growth exceeds the rate at which new words are appearing to the learner!'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.010619469026548672
Comparing: 
  Sentence: 'such facts have motivated the distributional hypothesis of chapter 6, which sug- gests that aspects of meaning can be learned solely from the texts we encounter over our lives, based on the complex association of words with the words they co-occur with (and with the words that those words occur with).'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.048411497730711045
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Such facts have motivated the distributional hypothesis of Chapter 6, which sug-
gests that aspects of meaning can be learned solely from the texts we encounter over
our lives, based on the complex association of words with the words they co-occur
with (and with the words that those words occur with).', Similarity: 0.048411497730711045
Comparing: 
  Sentence: 'the distributional hypothe- sis suggests both that we can acquire remarkable amounts of knowledge from text, and that this knowledge can be brought to bear long after its initial acquisition.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.01090909090909091
Comparing: 
  Sentence: 'of course, grounding from real-world interaction or other modalities can help build even more powerful models, but even text alone is remarkably useful.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.03913894324853229
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Of
course, grounding from real-world interaction or other modalities can help build
even more powerful models, but even text alone is remarkably useful.', Similarity: 0.03913894324853229
Comparing: 
  Sentence: 'in this chapter we formalize this idea of pretraining—learning knowledge about pretraining language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.014010507880910683
Comparing: 
  Sentence: 'large language models exhibit remark-'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.18181818181818182
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Large language models exhibit remark-', Similarity: 0.18181818181818182

Highlight Summary:
Total Highlights: 34
Highlighted: True
INFO:     49.207.220.184:29492 - "POST /query-pdf/ HTTP/1.1" 200 OK

--- Sentence Matching Process ---
Target Page: 1
Search Phrases: ['Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.']
Similarity Threshold: 0.03

Comparing: 
  Sentence: 'speech and language processing.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.005128205128205128
Comparing: 
  Sentence: 'daniel jurafsky & james h.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.005194805194805195
Comparing: 
  Sentence: 'martin.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.00546448087431694
Comparing: 
  Sentence: 'copyright © 2024.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.005319148936170213
Comparing: 
  Sentence: 'all rights reserved.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.0158311345646438
Comparing: 
  Sentence: 'draft of august 20, 2024.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.005208333333333333
Comparing: 
  Sentence: 'chapter 10 large language models “how much do we know at any time?'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.0
Comparing: 
  Sentence: 'much more, or so i believe, than we know we know.” agatha christie, the moving finger fluent speakers of a language bring an enormous amount of knowledge to bear dur- ing comprehension and production.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.01073345259391771
Comparing: 
  Sentence: 'this knowledge is embodied in many forms, perhaps most obviously in the vocabulary, the rich representations we have of words and their meanings and usage.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.011673151750972763
Comparing: 
  Sentence: 'this makes the vocabulary a useful lens to explore the acquisition of knowledge from text, by both people and machines.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.029288702928870293
Comparing: 
  Sentence: 'estimates of the size of adult vocabularies vary widely both within and across languages.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.017857142857142856
Comparing: 
  Sentence: 'for example, estimates of the vocabulary size of young adult speakers of american english range from 30,000 to 100,000 depending on the resources used to make the estimate and the deﬁnition of what it means to know a word.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.024096385542168676
Comparing: 
  Sentence: 'what is agreed upon is that the vast majority of words that mature speakers use in their day-to-day interactions are acquired early in life through spoken interactions with caregivers and peers, usually well before the start of formal schooling.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.013245033112582781
Comparing: 
  Sentence: 'this active vocabulary (usually on the order of 2000 words for young speakers) is extremely limited compared to the size of the adult vocabulary, and is quite stable, with very few additional words learned via casual conversation beyond this early stage.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.04241435562805873
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'This active
vocabulary (usually on the order of 2000 words for young speakers) is extremely
limited compared to the size of the adult vocabulary, and is quite stable, with very
few additional words learned via casual conversation beyond this early stage.', Similarity: 0.04241435562805873
Comparing: 
  Sentence: 'obvi- ously, this leaves a very large number of words to be acquired by other means.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.03160270880361174
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Obvi-
ously, this leaves a very large number of words to be acquired by other means.', Similarity: 0.03160270880361174
Comparing: 
  Sentence: 'a simple consequence of these facts is that children have to learn about 7 to 10 words a day, every single day, to arrive at observed vocabulary levels by the time they are 20 years of age.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.014598540145985401
Comparing: 
  Sentence: 'and indeed empirical estimates of vocabulary growth in late el- ementary through high school are consistent with this rate.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.008298755186721992
Comparing: 
  Sentence: 'how do children achieve this rate of vocabulary growth?'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.01932367149758454
Comparing: 
  Sentence: 'the bulk of this knowledge acquisition seems to happen as a by-product of reading, as part of the rich processing and reasoning that we perform when we read.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.011627906976744186
Comparing: 
  Sentence: 'research into the average amount of time children spend reading, and the lexical diversity of the texts they read, indicate that it is possible to achieve the desired rate.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.022598870056497175
Comparing: 
  Sentence: 'but the mechanism behind this rate of learning must be remarkable indeed, since at some points during learning the rate of vocabulary growth exceeds the rate at which new words are appearing to the learner!'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.010619469026548672
Comparing: 
  Sentence: 'such facts have motivated the distributional hypothesis of chapter 6, which sug- gests that aspects of meaning can be learned solely from the texts we encounter over our lives, based on the complex association of words with the words they co-occur with (and with the words that those words occur with).'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.048411497730711045
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Such facts have motivated the distributional hypothesis of Chapter 6, which sug-
gests that aspects of meaning can be learned solely from the texts we encounter over
our lives, based on the complex association of words with the words they co-occur
with (and with the words that those words occur with).', Similarity: 0.048411497730711045
Comparing: 
  Sentence: 'the distributional hypothe- sis suggests both that we can acquire remarkable amounts of knowledge from text, and that this knowledge can be brought to bear long after its initial acquisition.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.01090909090909091
Comparing: 
  Sentence: 'of course, grounding from real-world interaction or other modalities can help build even more powerful models, but even text alone is remarkably useful.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.03913894324853229
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Of
course, grounding from real-world interaction or other modalities can help build
even more powerful models, but even text alone is remarkably useful.', Similarity: 0.03913894324853229
Comparing: 
  Sentence: 'in this chapter we formalize this idea of pretraining—learning knowledge about pretraining language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.014010507880910683
Comparing: 
  Sentence: 'large language models exhibit remark-'
  Target:   'large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. they have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.'
  Similarity: 0.18181818181818182
Fuzzy match found! Phrase: 'Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.', Sentence: 'Large language models exhibit remark-', Similarity: 0.18181818181818182
Comparing: 
  Sentence: 'speech and language processing.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.05194805194805195
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Speech and Language Processing.', Similarity: 0.05194805194805195
Comparing: 
  Sentence: 'daniel jurafsky & james h.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.017699115044247787
Comparing: 
  Sentence: 'martin.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.00966183574879227
Comparing: 
  Sentence: 'copyright © 2024.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.027649769585253458
Comparing: 
  Sentence: 'all rights reserved.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.01818181818181818
Comparing: 
  Sentence: 'draft of august 20, 2024.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.008888888888888889
Comparing: 
  Sentence: 'chapter 10 large language models “how much do we know at any time?'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.07518796992481203
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'CHAPTER
10
Large Language Models
“How much do we know at any time?', Similarity: 0.07518796992481203
Comparing: 
  Sentence: 'much more, or so i believe, than we know we know.” agatha christie, the moving finger fluent speakers of a language bring an enormous amount of knowledge to bear dur- ing comprehension and production.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.025
Comparing: 
  Sentence: 'this knowledge is embodied in many forms, perhaps most obviously in the vocabulary, the rich representations we have of words and their meanings and usage.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.09014084507042254
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'This knowledge is embodied in many forms,
perhaps most obviously in the vocabulary, the rich representations we have of words
and their meanings and usage.', Similarity: 0.09014084507042254
Comparing: 
  Sentence: 'this makes the vocabulary a useful lens to explore the acquisition of knowledge from text, by both people and machines.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.050156739811912224
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'This makes the vocabulary a useful lens to explore
the acquisition of knowledge from text, by both people and machines.', Similarity: 0.050156739811912224
Comparing: 
  Sentence: 'estimates of the size of adult vocabularies vary widely both within and across languages.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.06920415224913495
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Estimates of the size of adult vocabularies vary widely both within and across
languages.', Similarity: 0.06920415224913495
Comparing: 
  Sentence: 'for example, estimates of the vocabulary size of young adult speakers of american english range from 30,000 to 100,000 depending on the resources used to make the estimate and the deﬁnition of what it means to know a word.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.018957345971563982
Comparing: 
  Sentence: 'what is agreed upon is that the vast majority of words that mature speakers use in their day-to-day interactions are acquired early in life through spoken interactions with caregivers and peers, usually well before the start of formal schooling.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.0449438202247191
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'What
is agreed upon is that the vast majority of words that mature speakers use in their
day-to-day interactions are acquired early in life through spoken interactions with
caregivers and peers, usually well before the start of formal schooling.', Similarity: 0.0449438202247191
Comparing: 
  Sentence: 'this active vocabulary (usually on the order of 2000 words for young speakers) is extremely limited compared to the size of the adult vocabulary, and is quite stable, with very few additional words learned via casual conversation beyond this early stage.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.02643171806167401
Comparing: 
  Sentence: 'obvi- ously, this leaves a very large number of words to be acquired by other means.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.035211267605633804
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Obvi-
ously, this leaves a very large number of words to be acquired by other means.', Similarity: 0.035211267605633804
Comparing: 
  Sentence: 'a simple consequence of these facts is that children have to learn about 7 to 10 words a day, every single day, to arrive at observed vocabulary levels by the time they are 20 years of age.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.02056555269922879
Comparing: 
  Sentence: 'and indeed empirical estimates of vocabulary growth in late el- ementary through high school are consistent with this rate.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.030959752321981424
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'And indeed empirical estimates of vocabulary growth in late el-
ementary through high school are consistent with this rate.', Similarity: 0.030959752321981424
Comparing: 
  Sentence: 'how do children achieve this rate of vocabulary growth?'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.023529411764705882
Comparing: 
  Sentence: 'the bulk of this knowledge acquisition seems to happen as a by-product of reading, as part of the rich processing and reasoning that we perform when we read.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.028011204481792718
Comparing: 
  Sentence: 'research into the average amount of time children spend reading, and the lexical diversity of the texts they read, indicate that it is possible to achieve the desired rate.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.03763440860215054
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Research into the average amount of time children spend
reading, and the lexical diversity of the texts they read, indicate that it is possible
to achieve the desired rate.', Similarity: 0.03763440860215054
Comparing: 
  Sentence: 'but the mechanism behind this rate of learning must be remarkable indeed, since at some points during learning the rate of vocabulary growth exceeds the rate at which new words are appearing to the learner!'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.019704433497536946
Comparing: 
  Sentence: 'such facts have motivated the distributional hypothesis of chapter 6, which sug- gests that aspects of meaning can be learned solely from the texts we encounter over our lives, based on the complex association of words with the words they co-occur with (and with the words that those words occur with).'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.0199203187250996
Comparing: 
  Sentence: 'the distributional hypothe- sis suggests both that we can acquire remarkable amounts of knowledge from text, and that this knowledge can be brought to bear long after its initial acquisition.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.015345268542199489
Comparing: 
  Sentence: 'of course, grounding from real-world interaction or other modalities can help build even more powerful models, but even text alone is remarkably useful.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.03409090909090909
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'Of
course, grounding from real-world interaction or other modalities can help build
even more powerful models, but even text alone is remarkably useful.', Similarity: 0.03409090909090909
Comparing: 
  Sentence: 'in this chapter we formalize this idea of pretraining—learning knowledge about pretraining language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.6213592233009708
Fuzzy match found! Phrase: 'In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.', Sentence: 'In this chapter we formalize this idea of pretraining—learning knowledge about
pretraining
language and the world from vast amounts of text—and call the resulting pretrained
language models large language models.', Similarity: 0.6213592233009708
Comparing: 
  Sentence: 'large language models exhibit remark-'
  Target:   'in this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.'
  Similarity: 0.016877637130801686

Highlight Summary:
Total Highlights: 34
Highlighted: True
INFO:     49.207.220.184:28704 - "POST /query-pdf/ HTTP/1.1" 200 OK
WARNING:  StatReload detected changes in 'app.py'. Reloading...
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [15400]
INFO:     Started server process [32109]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     49.207.220.184:28958 - "OPTIONS /query-pdf/ HTTP/1.1" 200 OK
INFO:     49.207.220.184:28959 - "POST /query-pdf/ HTTP/1.1" 500 Internal Server Error
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/python_backend/app.py", line 244, in query_pdf
    return pdf_assistant.query_pdf(request.question)
  File "/home/python_backend/app.py", line 183, in query_pdf
    highlighted_image = self._highlight_text(
  File "/home/python_backend/app.py", line 90, in _highlight_text
    img = img.filter(ImageEnhance.Sharpness(1.5))
  File "/home/python_backend/venv/lib/python3.10/site-packages/PIL/ImageEnhance.py", line 110, in __init__
    self.degenerate = image.filter(ImageFilter.SMOOTH)
AttributeError: 'float' object has no attribute 'filter'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/python_backend/venv/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py", line 403, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
  File "/home/python_backend/venv/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
  File "/home/python_backend/venv/lib/python3.10/site-packages/fastapi/applications.py", line 1054, in __call__
    await super().__call__(scope, receive, send)
  File "/home/python_backend/venv/lib/python3.10/site-packages/starlette/applications.py", line 113, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/python_backend/venv/lib/python3.10/site-packages/starlette/middleware/errors.py", line 187, in __call__
    raise exc
  File "/home/python_backend/venv/lib/python3.10/site-packages/starlette/middleware/errors.py", line 165, in __call__
    await self.app(scope, receive, _send)
  File "/home/python_backend/venv/lib/python3.10/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/home/python_backend/venv/lib/python3.10/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/home/python_backend/venv/lib/python3.10/site-packages/starlette/middleware/exceptions.py", line 62, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/python_backend/venv/lib/python3.10/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/python_backend/venv/lib/python3.10/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/python_backend/venv/lib/python3.10/site-packages/starlette/routing.py", line 715, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/python_backend/venv/lib/python3.10/site-packages/starlette/routing.py", line 735, in app
    await route.handle(scope, receive, send)
  File "/home/python_backend/venv/lib/python3.10/site-packages/starlette/routing.py", line 288, in handle
    await self.app(scope, receive, send)
  File "/home/python_backend/venv/lib/python3.10/site-packages/starlette/routing.py", line 76, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/python_backend/venv/lib/python3.10/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/python_backend/venv/lib/python3.10/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/python_backend/venv/lib/python3.10/site-packages/starlette/routing.py", line 73, in app
    response = await f(request)
  File "/home/python_backend/venv/lib/python3.10/site-packages/fastapi/routing.py", line 301, in app
    raw_response = await run_endpoint_function(
  File "/home/python_backend/venv/lib/python3.10/site-packages/fastapi/routing.py", line 212, in run_endpoint_function
    return await dependant.call(**values)
  File "/home/python_backend/app.py", line 251, in query_pdf
    highlighted_image = pdf_assistant._highlight_text(
  File "/home/python_backend/app.py", line 90, in _highlight_text
    img = img.filter(ImageEnhance.Sharpness(1.5))
  File "/home/python_backend/venv/lib/python3.10/site-packages/PIL/ImageEnhance.py", line 110, in __init__
    self.degenerate = image.filter(ImageFilter.SMOOTH)
AttributeError: 'float' object has no attribute 'filter'
INFO:     Will watch for changes in these directories: ['/home/python_backend']
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [46751] using StatReload
INFO:     Started server process [46770]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     171.76.85.255:21322 - "OPTIONS /query-pdf/ HTTP/1.1" 200 OK

--- Sentence Matching Process ---
Target Page: 2
Search Phrases: ['•']
Similarity Threshold: 0.03

Exact match found for phrase: '•'

Highlight Summary:
Total Highlights: 21
Highlighted: True
INFO:     171.76.85.255:16977 - "POST /query-pdf/ HTTP/1.1" 200 OK
INFO:     49.207.220.184:27745 - "OPTIONS /query-pdf/ HTTP/1.1" 200 OK

--- Sentence Matching Process ---
Target Page: 5
Search Phrases: ['Net product sales: $ 67,601\nNet service sales: $ 91,276\nTotal net sales: $ 158,877', 'Net sales increased 11% to $158.9 billion in the third quarter, compared with $143.1 billion in third quarter 2023.']
Similarity Threshold: 0.03

Comparing: 
  Sentence: 'amazon.com, inc.'
  Target:   'net product sales: $ 67,601 net service sales: $ 91,276 total net sales: $ 158,877'
  Similarity: 0.10204081632653061
Fuzzy match found! Phrase: 'Net product sales: $ 67,601
Net service sales: $ 91,276
Total net sales: $ 158,877', Sentence: 'AMAZON.COM, INC.', Similarity: 0.10204081632653061
Comparing: 
  Sentence: 'consolidated statements of operations (in millions, except per share data) (unaudited) three months ended september 30, nine months ended september 30, 2023 2024 2023 2024 net product sales $ 63,171 $ 67,601 $ 179,184 $ 190,085 net service sales 79,912 91,276 225,640 260,082 total net sales 143,083 158,877 404,824 450,167 operating expenses: cost of sales 75,022 80,977 212,186 227,395 fulfillment 22,314 24,660 64,524 70,543 technology and infrastructure 21,203 22,245 63,584 64,973 sales and marketing 10,551 10,609 31,468 30,783 general and administrative 2,561 2,713 8,806 8,496 other operating expense (income), net 244 262 613 587 total operating expenses 131,895 141,466 381,181 402,777 operating income 11,188 17,411 23,643 47,390 interest income 776 1,256 2,048 3,429 interest expense (806) (603) (2,469) (1,836) other income (expense), net 1,031 (27) 649 (2,718) total non-operating income (expense) 1,001 626 228 (1,125) income before income taxes 12,189 18,037 23,871 46,265 provision for income taxes (2,306) (2,706) (4,058) (6,940) equity-method investment activity, net of tax (4) (3) (12) (81) net income $ 9,879 $ 15,328 $ 19,801 $ 39,244 basic earnings per share $ 0.96 $ 1.46 $ 1.93 $ 3.76 diluted earnings per share $ 0.94 $ 1.43 $ 1.89 $ 3.67 weighted-average shares used in computation of earnings per share: basic 10,322 10,501 10,286 10,447 diluted 10,558 10,735 10,452 10,705'
  Target:   'net product sales: $ 67,601 net service sales: $ 91,276 total net sales: $ 158,877'
  Similarity: 0.10377358490566038
Fuzzy match found! Phrase: 'Net product sales: $ 67,601
Net service sales: $ 91,276
Total net sales: $ 158,877', Sentence: 'Consolidated Statements of Operations
(in millions, except per share data)
(unaudited)
  
Three Months Ended
September 30,
Nine Months Ended
September 30,
2023
2024
2023
2024
Net product sales
$ 
63,171 $ 
67,601 $ 
179,184 $ 
190,085 
Net service sales
 
79,912  
91,276  
225,640  
260,082 
Total net sales
 
143,083  
158,877  
404,824  
450,167 
Operating expenses:
Cost of sales
 
75,022  
80,977  
212,186  
227,395 
Fulfillment
 
22,314  
24,660  
64,524  
70,543 
Technology and infrastructure
 
21,203  
22,245  
63,584  
64,973 
Sales and marketing
 
10,551  
10,609  
31,468  
30,783 
General and administrative
 
2,561  
2,713  
8,806  
8,496 
Other operating expense (income), net
 
244  
262  
613  
587 
Total operating expenses
 
131,895  
141,466  
381,181  
402,777 
Operating income
 
11,188  
17,411  
23,643  
47,390 
Interest income
 
776  
1,256  
2,048  
3,429 
Interest expense
 
(806)  
(603)  
(2,469)  
(1,836) 
Other income (expense), net
 
1,031  
(27)  
649  
(2,718) 
Total non-operating income (expense)
 
1,001  
626  
228  
(1,125) 
Income before income taxes
 
12,189  
18,037  
23,871  
46,265 
Provision for income taxes
 
(2,306)  
(2,706)  
(4,058)  
(6,940) 
Equity-method investment activity, net of tax
 
(4)  
(3)  
(12)  
(81) 
Net income
$ 
9,879 $ 
15,328 $ 
19,801 $ 
39,244 
Basic earnings per share
$ 
0.96 $ 
1.46 $ 
1.93 $ 
3.76 
Diluted earnings per share
$ 
0.94 $ 
1.43 $ 
1.89 $ 
3.67 
Weighted-average shares used in computation of earnings per 
share:
Basic
 
10,322  
10,501  
10,286  
10,447 
Diluted
 
10,558  
10,735  
10,452  
10,705', Similarity: 0.10377358490566038
Comparing: 
  Sentence: 'amazon.com, inc.'
  Target:   'net sales increased 11% to $158.9 billion in the third quarter, compared with $143.1 billion in third quarter 2023.'
  Similarity: 0.0916030534351145
Fuzzy match found! Phrase: 'Net sales increased 11% to $158.9 billion in the third quarter, compared with $143.1 billion in third quarter 2023.', Sentence: 'AMAZON.COM, INC.', Similarity: 0.0916030534351145
Comparing: 
  Sentence: 'consolidated statements of operations (in millions, except per share data) (unaudited) three months ended september 30, nine months ended september 30, 2023 2024 2023 2024 net product sales $ 63,171 $ 67,601 $ 179,184 $ 190,085 net service sales 79,912 91,276 225,640 260,082 total net sales 143,083 158,877 404,824 450,167 operating expenses: cost of sales 75,022 80,977 212,186 227,395 fulfillment 22,314 24,660 64,524 70,543 technology and infrastructure 21,203 22,245 63,584 64,973 sales and marketing 10,551 10,609 31,468 30,783 general and administrative 2,561 2,713 8,806 8,496 other operating expense (income), net 244 262 613 587 total operating expenses 131,895 141,466 381,181 402,777 operating income 11,188 17,411 23,643 47,390 interest income 776 1,256 2,048 3,429 interest expense (806) (603) (2,469) (1,836) other income (expense), net 1,031 (27) 649 (2,718) total non-operating income (expense) 1,001 626 228 (1,125) income before income taxes 12,189 18,037 23,871 46,265 provision for income taxes (2,306) (2,706) (4,058) (6,940) equity-method investment activity, net of tax (4) (3) (12) (81) net income $ 9,879 $ 15,328 $ 19,801 $ 39,244 basic earnings per share $ 0.96 $ 1.46 $ 1.93 $ 3.76 diluted earnings per share $ 0.94 $ 1.43 $ 1.89 $ 3.67 weighted-average shares used in computation of earnings per share: basic 10,322 10,501 10,286 10,447 diluted 10,558 10,735 10,452 10,705'
  Target:   'net sales increased 11% to $158.9 billion in the third quarter, compared with $143.1 billion in third quarter 2023.'
  Similarity: 0.06987475280158208
Fuzzy match found! Phrase: 'Net sales increased 11% to $158.9 billion in the third quarter, compared with $143.1 billion in third quarter 2023.', Sentence: 'Consolidated Statements of Operations
(in millions, except per share data)
(unaudited)
  
Three Months Ended
September 30,
Nine Months Ended
September 30,
2023
2024
2023
2024
Net product sales
$ 
63,171 $ 
67,601 $ 
179,184 $ 
190,085 
Net service sales
 
79,912  
91,276  
225,640  
260,082 
Total net sales
 
143,083  
158,877  
404,824  
450,167 
Operating expenses:
Cost of sales
 
75,022  
80,977  
212,186  
227,395 
Fulfillment
 
22,314  
24,660  
64,524  
70,543 
Technology and infrastructure
 
21,203  
22,245  
63,584  
64,973 
Sales and marketing
 
10,551  
10,609  
31,468  
30,783 
General and administrative
 
2,561  
2,713  
8,806  
8,496 
Other operating expense (income), net
 
244  
262  
613  
587 
Total operating expenses
 
131,895  
141,466  
381,181  
402,777 
Operating income
 
11,188  
17,411  
23,643  
47,390 
Interest income
 
776  
1,256  
2,048  
3,429 
Interest expense
 
(806)  
(603)  
(2,469)  
(1,836) 
Other income (expense), net
 
1,031  
(27)  
649  
(2,718) 
Total non-operating income (expense)
 
1,001  
626  
228  
(1,125) 
Income before income taxes
 
12,189  
18,037  
23,871  
46,265 
Provision for income taxes
 
(2,306)  
(2,706)  
(4,058)  
(6,940) 
Equity-method investment activity, net of tax
 
(4)  
(3)  
(12)  
(81) 
Net income
$ 
9,879 $ 
15,328 $ 
19,801 $ 
39,244 
Basic earnings per share
$ 
0.96 $ 
1.46 $ 
1.93 $ 
3.76 
Diluted earnings per share
$ 
0.94 $ 
1.43 $ 
1.89 $ 
3.67 
Weighted-average shares used in computation of earnings per 
share:
Basic
 
10,322  
10,501  
10,286  
10,447 
Diluted
 
10,558  
10,735  
10,452  
10,705', Similarity: 0.06987475280158208

Highlight Summary:
Total Highlights: 448
Highlighted: True
INFO:     49.207.220.184:27745 - "POST /query-pdf/ HTTP/1.1" 200 OK
INFO:     154.213.184.18:43242 - "CONNECT example.com%3A443 HTTP/1.1" 404 Not Found
INFO:     Will watch for changes in these directories: ['/home/python_backend']
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [37916] using StatReload
INFO:     Started server process [37925]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     49.207.225.54:61214 - "OPTIONS /query-pdf/ HTTP/1.1" 200 OK

--- Sentence Matching Process ---
Target Page: 1
Search Phrases: ['Net sales increased 11% to $158.9 billion in the third quarter, compared with $143.1 billion in third quarter 2023.', 'References to customers mean customer accounts established when a customer places an order through one of our stores. Customer accounts exclude certain customers, including customers associated with certain of our acquisitions, Amazon Payments customers, AWS customers, and the customers of select companies with whom we have a technology alliance or marketing and promotional relationship. Customers are considered active when they have placed an order during the preceding twelve-month period.']
Similarity Threshold: 0.03

Exact match found for phrase: 'Net sales increased 11% to $158.9 billion in the third quarter, compared with $143.1 billion in third quarter 2023.'

Highlight Summary:
Total Highlights: 1
Highlighted: True
INFO:     49.207.225.54:61215 - "POST /query-pdf/ HTTP/1.1" 200 OK

--- Sentence Matching Process ---
Target Page: 12
Search Phrases: ['References to sellers means seller accounts, which are established when a seller receives an order from a customer account. Sellers are considered active when they have received an order from a customer during the preceding twelve-month period.']
Similarity Threshold: 0.03

Comparing: 
  Sentence: 'amazon.com, inc.'
  Target:   'references to sellers means seller accounts, which are established when a seller receives an order from a customer account. sellers are considered active when they have received an order from a customer during the preceding twelve-month period.'
  Similarity: 0.015384615384615385
Comparing: 
  Sentence: 'certain definitions customer accounts • references to customers mean customer accounts established when a customer places an order through one of our stores.'
  Target:   'references to sellers means seller accounts, which are established when a seller receives an order from a customer account. sellers are considered active when they have received an order from a customer during the preceding twelve-month period.'
  Similarity: 0.034912718204488775
Fuzzy match found! Phrase: 'References to sellers means seller accounts, which are established when a seller receives an order from a customer account. Sellers are considered active when they have received an order from a customer during the preceding twelve-month period.', Sentence: 'Certain Definitions
Customer Accounts
•
References to customers mean customer accounts established when a customer places an order through one of our 
stores.', Similarity: 0.034912718204488775
Comparing: 
  Sentence: 'customer accounts exclude certain customers, including customers associated with certain of our acquisitions, amazon payments customers, aws customers, and the customers of select companies with whom we have a technology alliance or marketing and promotional relationship.'
  Target:   'references to sellers means seller accounts, which are established when a seller receives an order from a customer account. sellers are considered active when they have received an order from a customer during the preceding twelve-month period.'
  Similarity: 0.04263565891472868
Fuzzy match found! Phrase: 'References to sellers means seller accounts, which are established when a seller receives an order from a customer account. Sellers are considered active when they have received an order from a customer during the preceding twelve-month period.', Sentence: 'Customer accounts exclude certain customers, including customers associated with certain of our acquisitions, 
Amazon Payments customers, AWS customers, and the customers of select companies with whom we have a 
technology alliance or marketing and promotional relationship.', Similarity: 0.04263565891472868
Comparing: 
  Sentence: 'customers are considered active when they have placed an order during the preceding twelve-month period.'
  Target:   'references to sellers means seller accounts, which are established when a seller receives an order from a customer account. sellers are considered active when they have received an order from a customer during the preceding twelve-month period.'
  Similarity: 0.3160919540229885
Fuzzy match found! Phrase: 'References to sellers means seller accounts, which are established when a seller receives an order from a customer account. Sellers are considered active when they have received an order from a customer during the preceding twelve-month period.', Sentence: 'Customers are considered active when they have 
placed an order during the preceding twelve-month period.', Similarity: 0.3160919540229885
Comparing: 
  Sentence: 'seller accounts • references to sellers means seller accounts, which are established when a seller receives an order from a customer account.'
  Target:   'references to sellers means seller accounts, which are established when a seller receives an order from a customer account. sellers are considered active when they have received an order from a customer during the preceding twelve-month period.'
  Similarity: 0.638961038961039
Fuzzy match found! Phrase: 'References to sellers means seller accounts, which are established when a seller receives an order from a customer account. Sellers are considered active when they have received an order from a customer during the preceding twelve-month period.', Sentence: 'Seller Accounts
•
References to sellers means seller accounts, which are established when a seller receives an order from a customer 
account.', Similarity: 0.638961038961039
Comparing: 
  Sentence: 'sellers are considered active when they have received an order from a customer during the preceding twelve- month period.'
  Target:   'references to sellers means seller accounts, which are established when a seller receives an order from a customer account. sellers are considered active when they have received an order from a customer during the preceding twelve-month period.'
  Similarity: 0.6575342465753424
Fuzzy match found! Phrase: 'References to sellers means seller accounts, which are established when a seller receives an order from a customer account. Sellers are considered active when they have received an order from a customer during the preceding twelve-month period.', Sentence: 'Sellers are considered active when they have received an order from a customer during the preceding twelve-
month period.', Similarity: 0.6575342465753424
Comparing: 
  Sentence: 'aws customers • references to aws customers mean unique aws customer accounts, which are unique customer account ids that are eligible to use aws services.'
  Target:   'references to sellers means seller accounts, which are established when a seller receives an order from a customer account. sellers are considered active when they have received an order from a customer during the preceding twelve-month period.'
  Similarity: 0.19548872180451127
Fuzzy match found! Phrase: 'References to sellers means seller accounts, which are established when a seller receives an order from a customer account. Sellers are considered active when they have received an order from a customer during the preceding twelve-month period.', Sentence: 'AWS Customers
•
References to AWS customers mean unique AWS customer accounts, which are unique customer account IDs that are 
eligible to use AWS services.', Similarity: 0.19548872180451127
Comparing: 
  Sentence: 'this includes aws accounts in the aws free tier.'
  Target:   'references to sellers means seller accounts, which are established when a seller receives an order from a customer account. sellers are considered active when they have received an order from a customer during the preceding twelve-month period.'
  Similarity: 0.0136986301369863
Comparing: 
  Sentence: 'multiple users accessing aws services via one account id are counted as a single account.'
  Target:   'references to sellers means seller accounts, which are established when a seller receives an order from a customer account. sellers are considered active when they have received an order from a customer during the preceding twelve-month period.'
  Similarity: 0.036036036036036036
Fuzzy match found! Phrase: 'References to sellers means seller accounts, which are established when a seller receives an order from a customer account. Sellers are considered active when they have received an order from a customer during the preceding twelve-month period.', Sentence: 'Multiple users accessing AWS 
services via one account ID are counted as a single account.', Similarity: 0.036036036036036036
Comparing: 
  Sentence: 'customers are considered active when they have had aws usage activity during the preceding one-month period.'
  Target:   'references to sellers means seller accounts, which are established when a seller receives an order from a customer account. sellers are considered active when they have received an order from a customer during the preceding twelve-month period.'
  Similarity: 0.3125
Fuzzy match found! Phrase: 'References to sellers means seller accounts, which are established when a seller receives an order from a customer account. Sellers are considered active when they have received an order from a customer during the preceding twelve-month period.', Sentence: 'Customers are considered active when they have had 
AWS usage activity during the preceding one-month period.', Similarity: 0.3125
Comparing: 
  Sentence: 'units • references to units mean physical and digital units sold (net of returns and cancellations) by us and sellers in our stores as well as amazon-owned items sold in other stores.'
  Target:   'references to sellers means seller accounts, which are established when a seller receives an order from a customer account. sellers are considered active when they have received an order from a customer during the preceding twelve-month period.'
  Similarity: 0.08899297423887588
Fuzzy match found! Phrase: 'References to sellers means seller accounts, which are established when a seller receives an order from a customer account. Sellers are considered active when they have received an order from a customer during the preceding twelve-month period.', Sentence: 'Units
•
References to units mean physical and digital units sold (net of returns and cancellations) by us and sellers in our 
stores as well as Amazon-owned items sold in other stores.', Similarity: 0.08899297423887588
Comparing: 
  Sentence: 'units sold are paid units and do not include units associated with aws, certain acquisitions, certain subscriptions, rental businesses, or advertising businesses, or amazon gift cards.'
  Target:   'references to sellers means seller accounts, which are established when a seller receives an order from a customer account. sellers are considered active when they have received an order from a customer during the preceding twelve-month period.'
  Similarity: 0.02336448598130841
Comparing: 
  Sentence: 'contacts: amazon investor relations amazon public relations amazon-ir@amazon.com amazon-pr@amazon.com amazon.com/ir amazon.com/pr'
  Target:   'references to sellers means seller accounts, which are established when a seller receives an order from a customer account. sellers are considered active when they have received an order from a customer during the preceding twelve-month period.'
  Similarity: 0.021447721179624665

Highlight Summary:
Total Highlights: 26
Highlighted: True
INFO:     49.207.225.54:61277 - "POST /query-pdf/ HTTP/1.1" 200 OK

--- Sentence Matching Process ---
Target Page: 11
Search Phrases: ['References to customers mean customer accounts established when a customer places an order through one of our stores.', 'Customers are considered active when they have placed an order during the preceding twelve-month period.']
Similarity Threshold: 0.03

Comparing: 
  Sentence: 'amazon.com, inc.'
  Target:   'references to customers mean customer accounts established when a customer places an order through one of our stores.'
  Similarity: 0.09022556390977443
Fuzzy match found! Phrase: 'References to customers mean customer accounts established when a customer places an order through one of our stores.', Sentence: 'AMAZON.COM, INC.', Similarity: 0.09022556390977443
Comparing: 
  Sentence: 'supplemental financial information and business metrics (in millions, except employee data) (unaudited) q2 2023 q3 2023 q4 2023 q1 2024 q2 2024 q3 2024 y/y % change net sales online stores (1) $ 52,966 $ 57,267 $ 70,543 $ 54,670 $ 55,392 $ 61,411 7 % online stores -- y/y growth, excluding f/x 5 % 6 % 8 % 7 % 6 % 8 % n/a physical stores (2) $ 5,024 $ 4,959 $ 5,152 $ 5,202 $ 5,206 $ 5,228 5 % physical stores -- y/y growth, excluding f/x 7 % 6 % 4 % 6 % 4 % 5 % n/a third-party seller services (3) $ 32,332 $ 34,342 $ 43,559 $ 34,596 $ 36,201 $ 37,864 10 % third-party seller services -- y/y growth, excluding f/x 18 % 18 % 19 % 16 % 13 % 10 % n/a advertising services (4) $ 10,683 $ 12,060 $ 14,654 $ 11,824 $ 12,771 $ 14,331 19 % advertising services -- y/y growth, excluding f/x 22 % 25 % 26 % 24 % 20 % 19 % n/a subscription services (5) $ 9,894 $ 10,170 $ 10,488 $ 10,722 $ 10,866 $ 11,278 11 % subscription services -- y/y growth, excluding f/x 14 % 13 % 13 % 11 % 11 % 11 % n/a aws $ 22,140 $ 23,059 $ 24,204 $ 25,037 $ 26,281 $ 27,452 19 % aws -- y/y growth, excluding f/x 12 % 12 % 13 % 17 % 19 % 19 % n/a other (6) $ 1,344 $ 1,226 $ 1,361 $ 1,262 $ 1,260 $ 1,313 7 % other -- y/y growth (decline), excluding f/x 26 % (3) % 8 % 23 % (6) % 5 % n/a stock-based compensation expense cost of sales $ 251 $ 193 $ 227 $ 174 $ 266 $ 193 (1) % fulfillment $ 932 $ 732 $ 823 $ 636 $ 944 $ 696 (5) % technology and infrastructure $ 4,043 $ 3,284 $ 3,533 $ 2,772 $ 3,670 $ 2,961 (10) % sales and marketing $ 1,303 $ 1,111 $ 1,216 $ 932 $ 1,224 $ 1,012 (9) % general and administrative $ 598 $ 509 $ 520 $ 447 $ 618 $ 471 (8) % total stock-based compensation expense $ 7,127 $ 5,829 $ 6,319 $ 4,961 $ 6,722 $ 5,333 (9) % other ww shipping costs $ 20,418 $ 21,799 $ 27,326 $ 21,834 $ 21,965 $ 23,501 8 % ww shipping costs -- y/y growth 6 % 9 % 11 % 10 % 8 % 8 % n/a ww paid units -- y/y growth (7) 9 % 9 % 12 % 12 % 11 % 12 % n/a ww seller unit mix -- % of ww paid units (7) 60 % 60 % 61 % 61 % 61 % 60 % n/a employees (full-time and part-time; excludes contractors & temporary personnel) 1,461,000 1,500,000 1,525,000 1,521,000 1,532,000 1,551,000 3 % employees (full-time and part-time; excludes contractors & temporary personnel) -- y/y growth (decline) (4) % (3) % (1) % 4 % 5 % 3 % n/a ________________________ (1) includes product sales and digital media content where we record revenue gross.'
  Target:   'references to customers mean customer accounts established when a customer places an order through one of our stores.'
  Similarity: 0.029446876243533624
Comparing: 
  Sentence: 'we leverage our retail infrastructure to offer a wide selection of consumable and durable goods that includes media products available in both a physical and digital format, such as books, videos, games, music, and software.'
  Target:   'references to customers mean customer accounts established when a customer places an order through one of our stores.'
  Similarity: 0.07038123167155426
Fuzzy match found! Phrase: 'References to customers mean customer accounts established when a customer places an order through one of our stores.', Sentence: 'We leverage our retail infrastructure to offer a wide selection of consumable and durable 
goods that includes media products available in both a physical and digital format, such as books, videos, games, music, and software.', Similarity: 0.07038123167155426
Comparing: 
  Sentence: 'these product sales include digital products sold on a transactional basis.'
  Target:   'references to customers mean customer accounts established when a customer places an order through one of our stores.'
  Similarity: 0.3854166666666667
Fuzzy match found! Phrase: 'References to customers mean customer accounts established when a customer places an order through one of our stores.', Sentence: 'These product sales include digital 
products sold on a transactional basis.', Similarity: 0.3854166666666667
Comparing: 
  Sentence: 'digital media content subscriptions that provide unlimited viewing or usage rights are included in “subscription services.” (2) includes product sales where our customers physically select items in a store.'
  Target:   'references to customers mean customer accounts established when a customer places an order through one of our stores.'
  Similarity: 0.22291021671826625
Fuzzy match found! Phrase: 'References to customers mean customer accounts established when a customer places an order through one of our stores.', Sentence: 'Digital media content subscriptions that provide unlimited viewing or usage rights are included in “Subscription services.” 
(2)
Includes product sales where our customers physically select items in a store.', Similarity: 0.22291021671826625
Comparing: 
  Sentence: 'sales to customers who order goods online for delivery or pickup at our physical stores are included in “online stores.” (3) includes commissions and any related fulfillment and shipping fees, and other third-party seller services.'
  Target:   'references to customers mean customer accounts established when a customer places an order through one of our stores.'
  Similarity: 0.25862068965517243
Fuzzy match found! Phrase: 'References to customers mean customer accounts established when a customer places an order through one of our stores.', Sentence: 'Sales to customers who order goods online for delivery or pickup at our physical stores are 
included in “Online stores.” 
(3)
Includes commissions and any related fulfillment and shipping fees, and other third-party seller services.', Similarity: 0.25862068965517243
Comparing: 
  Sentence: '(4) includes sales of advertising services to sellers, vendors, publishers, authors, and others, through programs such as sponsored ads, display, and video advertising.'
  Target:   'references to customers mean customer accounts established when a customer places an order through one of our stores.'
  Similarity: 0.42105263157894735
Fuzzy match found! Phrase: 'References to customers mean customer accounts established when a customer places an order through one of our stores.', Sentence: '(4)
Includes sales of advertising services to sellers, vendors, publishers, authors, and others, through programs such as sponsored ads, display, and video advertising.', Similarity: 0.42105263157894735
Comparing: 
  Sentence: '(5) includes annual and monthly fees associated with amazon prime memberships, as well as digital video, audiobook, digital music, e-book, and other non-aws subscription services.'
  Target:   'references to customers mean customer accounts established when a customer places an order through one of our stores.'
  Similarity: 0.14864864864864866
Fuzzy match found! Phrase: 'References to customers mean customer accounts established when a customer places an order through one of our stores.', Sentence: '(5)
Includes annual and monthly fees associated with Amazon Prime memberships, as well as digital video, audiobook, digital music, e-book, and other non-AWS subscription 
services.', Similarity: 0.14864864864864866
Comparing: 
  Sentence: '(6) includes sales related to various other offerings, such as health care services, certain licensing and distribution of video content, and shipping services, and our co-branded credit card agreements.'
  Target:   'references to customers mean customer accounts established when a customer places an order through one of our stores.'
  Similarity: 0.2875
Fuzzy match found! Phrase: 'References to customers mean customer accounts established when a customer places an order through one of our stores.', Sentence: '(6)
Includes sales related to various other offerings, such as health care services, certain licensing and distribution of video content, and shipping services, and our co-branded 
credit card agreements.', Similarity: 0.2875
Comparing: 
  Sentence: '(7) excludes the impact of whole foods market.'
  Target:   'references to customers mean customer accounts established when a customer places an order through one of our stores.'
  Similarity: 0.26993865030674846
Fuzzy match found! Phrase: 'References to customers mean customer accounts established when a customer places an order through one of our stores.', Sentence: '(7)
Excludes the impact of Whole Foods Market.', Similarity: 0.26993865030674846
Comparing: 
  Sentence: ''
  Target:   'references to customers mean customer accounts established when a customer places an order through one of our stores.'
  Similarity: 0.0
Comparing: 
  Sentence: 'amazon.com, inc.'
  Target:   'customers are considered active when they have placed an order during the preceding twelve-month period.'
  Similarity: 0.16666666666666666
Fuzzy match found! Phrase: 'Customers are considered active when they have placed an order during the preceding twelve-month period.', Sentence: 'AMAZON.COM, INC.', Similarity: 0.16666666666666666
Comparing: 
  Sentence: 'supplemental financial information and business metrics (in millions, except employee data) (unaudited) q2 2023 q3 2023 q4 2023 q1 2024 q2 2024 q3 2024 y/y % change net sales online stores (1) $ 52,966 $ 57,267 $ 70,543 $ 54,670 $ 55,392 $ 61,411 7 % online stores -- y/y growth, excluding f/x 5 % 6 % 8 % 7 % 6 % 8 % n/a physical stores (2) $ 5,024 $ 4,959 $ 5,152 $ 5,202 $ 5,206 $ 5,228 5 % physical stores -- y/y growth, excluding f/x 7 % 6 % 4 % 6 % 4 % 5 % n/a third-party seller services (3) $ 32,332 $ 34,342 $ 43,559 $ 34,596 $ 36,201 $ 37,864 10 % third-party seller services -- y/y growth, excluding f/x 18 % 18 % 19 % 16 % 13 % 10 % n/a advertising services (4) $ 10,683 $ 12,060 $ 14,654 $ 11,824 $ 12,771 $ 14,331 19 % advertising services -- y/y growth, excluding f/x 22 % 25 % 26 % 24 % 20 % 19 % n/a subscription services (5) $ 9,894 $ 10,170 $ 10,488 $ 10,722 $ 10,866 $ 11,278 11 % subscription services -- y/y growth, excluding f/x 14 % 13 % 13 % 11 % 11 % 11 % n/a aws $ 22,140 $ 23,059 $ 24,204 $ 25,037 $ 26,281 $ 27,452 19 % aws -- y/y growth, excluding f/x 12 % 12 % 13 % 17 % 19 % 19 % n/a other (6) $ 1,344 $ 1,226 $ 1,361 $ 1,262 $ 1,260 $ 1,313 7 % other -- y/y growth (decline), excluding f/x 26 % (3) % 8 % 23 % (6) % 5 % n/a stock-based compensation expense cost of sales $ 251 $ 193 $ 227 $ 174 $ 266 $ 193 (1) % fulfillment $ 932 $ 732 $ 823 $ 636 $ 944 $ 696 (5) % technology and infrastructure $ 4,043 $ 3,284 $ 3,533 $ 2,772 $ 3,670 $ 2,961 (10) % sales and marketing $ 1,303 $ 1,111 $ 1,216 $ 932 $ 1,224 $ 1,012 (9) % general and administrative $ 598 $ 509 $ 520 $ 447 $ 618 $ 471 (8) % total stock-based compensation expense $ 7,127 $ 5,829 $ 6,319 $ 4,961 $ 6,722 $ 5,333 (9) % other ww shipping costs $ 20,418 $ 21,799 $ 27,326 $ 21,834 $ 21,965 $ 23,501 8 % ww shipping costs -- y/y growth 6 % 9 % 11 % 10 % 8 % 8 % n/a ww paid units -- y/y growth (7) 9 % 9 % 12 % 12 % 11 % 12 % n/a ww seller unit mix -- % of ww paid units (7) 60 % 60 % 61 % 61 % 61 % 60 % n/a employees (full-time and part-time; excludes contractors & temporary personnel) 1,461,000 1,500,000 1,525,000 1,521,000 1,532,000 1,551,000 3 % employees (full-time and part-time; excludes contractors & temporary personnel) -- y/y growth (decline) (4) % (3) % (1) % 4 % 5 % 3 % n/a ________________________ (1) includes product sales and digital media content where we record revenue gross.'
  Target:   'customers are considered active when they have placed an order during the preceding twelve-month period.'
  Similarity: 0.0328
Fuzzy match found! Phrase: 'Customers are considered active when they have placed an order during the preceding twelve-month period.', Sentence: 'Supplemental Financial Information and Business Metrics
(in millions, except employee data)
(unaudited)
Q2 2023
Q3 2023
Q4 2023
Q1 2024
Q2 2024
Q3 2024
Y/Y %
Change
Net Sales
Online stores (1)
$ 52,966 
$ 57,267 
$ 70,543 
$ 54,670 
$ 55,392 
$ 61,411 
 7 %
Online stores -- Y/Y growth, excluding F/X
 5 %
 6 %
 8 %
 7 %
 6 %
 8 %
N/A
Physical stores (2)
$ 5,024 
$ 4,959 
$ 5,152 
$ 5,202 
$ 5,206 
$ 5,228 
 5 %
Physical stores -- Y/Y growth, excluding F/X
 7 %
 6 %
 4 %
 6 %
 4 %
 5 %
N/A
Third-party seller services (3)
$ 32,332 
$ 34,342 
$ 43,559 
$ 34,596 
$ 36,201 
$ 37,864 
 10 %
Third-party seller services -- Y/Y growth, excluding F/X
 18 %
 18 %
 19 %
 16 %
 13 %
 10 %
N/A
Advertising services (4)
$ 10,683 
$ 12,060 
$ 14,654 
$ 11,824 
$ 12,771 
$ 14,331 
 19 %
Advertising services -- Y/Y growth, excluding F/X
 22 %
 25 %
 26 %
 24 %
 20 %
 19 %
N/A
Subscription services (5)
$ 9,894 
$ 10,170 
$ 10,488 
$ 10,722 
$ 10,866 
$ 11,278 
 11 %
Subscription services -- Y/Y growth, excluding F/X
 14 %
 13 %
 13 %
 11 %
 11 %
 11 %
N/A
AWS
$ 22,140 
$ 23,059 
$ 24,204 
$ 25,037 
$ 26,281 
$ 27,452 
 19 %
AWS -- Y/Y growth, excluding F/X
 12 %
 12 %
 13 %
 17 %
 19 %
 19 %
N/A
Other (6)
$ 1,344 
$ 1,226 
$ 1,361 
$ 1,262 
$ 1,260 
$ 1,313 
 7 %
Other -- Y/Y growth (decline), excluding F/X
 26 %
 (3) %
 8 %
 23 %
 (6) %
 5 %
N/A
Stock-based Compensation Expense
Cost of sales
$ 
251 
$ 
193 
$ 
227 
$ 
174 
$ 
266 
$ 
193 
 (1) %
Fulfillment
$ 
932 
$ 
732 
$ 
823 
$ 
636 
$ 
944 
$ 
696 
 (5) %
Technology and infrastructure
$ 4,043 
$ 3,284 
$ 3,533 
$ 2,772 
$ 3,670 
$ 2,961 
 (10) %
Sales and marketing
$ 1,303 
$ 1,111 
$ 1,216 
$ 
932 
$ 1,224 
$ 1,012 
 (9) %
General and administrative
$ 
598 
$ 
509 
$ 
520 
$ 
447 
$ 
618 
$ 
471 
 (8) %
Total stock-based compensation expense
$ 7,127 
$ 5,829 
$ 6,319 
$ 4,961 
$ 6,722 
$ 5,333 
 (9) %
Other
WW shipping costs
$ 20,418 
$ 21,799 
$ 27,326 
$ 21,834 
$ 21,965 
$ 23,501 
 8 %
WW shipping costs -- Y/Y growth
 6 %
 9 %
 11 %
 10 %
 8 %
 8 %
N/A
WW paid units -- Y/Y growth (7)
 9 %
 9 %
 12 %
 12 %
 11 %
 12 %
N/A
WW seller unit mix -- % of WW paid units (7)
 60 %
 60 %
 61 %
 61 %
 61 %
 60 %
N/A
Employees (full-time and part-time; excludes contractors & temporary personnel)
 1,461,000 
 1,500,000 
 1,525,000 
 1,521,000 
 1,532,000 
 1,551,000 
 3 %
Employees (full-time and part-time; excludes contractors & temporary personnel) -- Y/Y 
growth (decline)
 (4) %
 (3) %
 (1) %
 4 %
 5 %
 3 %
N/A
________________________
(1)
Includes product sales and digital media content where we record revenue gross.', Similarity: 0.0328
Comparing: 
  Sentence: 'we leverage our retail infrastructure to offer a wide selection of consumable and durable goods that includes media products available in both a physical and digital format, such as books, videos, games, music, and software.'
  Target:   'customers are considered active when they have placed an order during the preceding twelve-month period.'
  Similarity: 0.24390243902439024
Fuzzy match found! Phrase: 'Customers are considered active when they have placed an order during the preceding twelve-month period.', Sentence: 'We leverage our retail infrastructure to offer a wide selection of consumable and durable 
goods that includes media products available in both a physical and digital format, such as books, videos, games, music, and software.', Similarity: 0.24390243902439024
Comparing: 
  Sentence: 'these product sales include digital products sold on a transactional basis.'
  Target:   'customers are considered active when they have placed an order during the preceding twelve-month period.'
  Similarity: 0.11173184357541899
Fuzzy match found! Phrase: 'Customers are considered active when they have placed an order during the preceding twelve-month period.', Sentence: 'These product sales include digital 
products sold on a transactional basis.', Similarity: 0.11173184357541899
Comparing: 
  Sentence: 'digital media content subscriptions that provide unlimited viewing or usage rights are included in “subscription services.” (2) includes product sales where our customers physically select items in a store.'
  Target:   'customers are considered active when they have placed an order during the preceding twelve-month period.'
  Similarity: 0.14838709677419354
Fuzzy match found! Phrase: 'Customers are considered active when they have placed an order during the preceding twelve-month period.', Sentence: 'Digital media content subscriptions that provide unlimited viewing or usage rights are included in “Subscription services.” 
(2)
Includes product sales where our customers physically select items in a store.', Similarity: 0.14838709677419354
Comparing: 
  Sentence: 'sales to customers who order goods online for delivery or pickup at our physical stores are included in “online stores.” (3) includes commissions and any related fulfillment and shipping fees, and other third-party seller services.'
  Target:   'customers are considered active when they have placed an order during the preceding twelve-month period.'
  Similarity: 0.22686567164179106
Fuzzy match found! Phrase: 'Customers are considered active when they have placed an order during the preceding twelve-month period.', Sentence: 'Sales to customers who order goods online for delivery or pickup at our physical stores are 
included in “Online stores.” 
(3)
Includes commissions and any related fulfillment and shipping fees, and other third-party seller services.', Similarity: 0.22686567164179106
Comparing: 
  Sentence: '(4) includes sales of advertising services to sellers, vendors, publishers, authors, and others, through programs such as sponsored ads, display, and video advertising.'
  Target:   'customers are considered active when they have placed an order during the preceding twelve-month period.'
  Similarity: 0.27941176470588236
Fuzzy match found! Phrase: 'Customers are considered active when they have placed an order during the preceding twelve-month period.', Sentence: '(4)
Includes sales of advertising services to sellers, vendors, publishers, authors, and others, through programs such as sponsored ads, display, and video advertising.', Similarity: 0.27941176470588236
Comparing: 
  Sentence: '(5) includes annual and monthly fees associated with amazon prime memberships, as well as digital video, audiobook, digital music, e-book, and other non-aws subscription services.'
  Target:   'customers are considered active when they have placed an order during the preceding twelve-month period.'
  Similarity: 0.18374558303886926
Fuzzy match found! Phrase: 'Customers are considered active when they have placed an order during the preceding twelve-month period.', Sentence: '(5)
Includes annual and monthly fees associated with Amazon Prime memberships, as well as digital video, audiobook, digital music, e-book, and other non-AWS subscription 
services.', Similarity: 0.18374558303886926
Comparing: 
  Sentence: '(6) includes sales related to various other offerings, such as health care services, certain licensing and distribution of video content, and shipping services, and our co-branded credit card agreements.'
  Target:   'customers are considered active when they have placed an order during the preceding twelve-month period.'
  Similarity: 0.2931596091205212
Fuzzy match found! Phrase: 'Customers are considered active when they have placed an order during the preceding twelve-month period.', Sentence: '(6)
Includes sales related to various other offerings, such as health care services, certain licensing and distribution of video content, and shipping services, and our co-branded 
credit card agreements.', Similarity: 0.2931596091205212
Comparing: 
  Sentence: '(7) excludes the impact of whole foods market.'
  Target:   'customers are considered active when they have placed an order during the preceding twelve-month period.'
  Similarity: 0.22666666666666666
Fuzzy match found! Phrase: 'Customers are considered active when they have placed an order during the preceding twelve-month period.', Sentence: '(7)
Excludes the impact of Whole Foods Market.', Similarity: 0.22666666666666666
Comparing: 
  Sentence: ''
  Target:   'customers are considered active when they have placed an order during the preceding twelve-month period.'
  Similarity: 0.0

Highlight Summary:
Total Highlights: 319
Highlighted: True

--- Sentence Matching Process ---
Target Page: 1
Search Phrases: ['Net sales increased 11% to $158.9 billion in the third quarter, compared with $143.1 billion in third quarter 2023.']
Similarity Threshold: 0.03

Exact match found for phrase: 'Net sales increased 11% to $158.9 billion in the third quarter, compared with $143.1 billion in third quarter 2023.'

Highlight Summary:
Total Highlights: 1
Highlighted: True
INFO:     49.207.225.54:61352 - "POST /query-pdf/ HTTP/1.1" 200 OK
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.14:46456 - "GET /favicon.ico HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.14:37778 - "GET /manage/account/login HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.14:39170 - "GET /admin/index.html HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.14:51868 - "GET /index.html HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.14:37208 - "GET /%2BCSCOE%2B/logon.html HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.14:59198 - "GET /cgi-bin/login.cgi HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.14:46882 - "GET /logon.htm HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.14:33090 - "GET /login.jsp HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.14:47274 - "GET /doc/index.html HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.14:54628 - "GET / HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     206.168.34.216:37800 - "GET / HTTP/1.1" 404 Not Found
INFO:     206.168.34.216:55004 - "GET / HTTP/1.1" 404 Not Found
INFO:     206.168.34.216:55022 - "PRI %2A HTTP/2.0" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     64.62.197.163:39171 - "GET / HTTP/1.1" 404 Not Found
INFO:     64.62.197.153:38027 - "GET /favicon.ico HTTP/1.1" 404 Not Found
INFO:     64.62.197.152:17349 - "GET http%3A//api.ipify.org/?format=json HTTP/1.1" 404 Not Found
INFO:     64.62.197.155:30589 - "CONNECT www.shadowserver.org%3A443 HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     171.76.84.160:26859 - "OPTIONS /query-pdf/ HTTP/1.1" 200 OK

--- Sentence Matching Process ---
Target Page: 9
Search Phrases: ['Operating cash flow -- trailing twelve months (TTM): $112,706', 'Free cash flow -- TTM: $47,747', 'Free cash flow less principal repayments of finance leases and financing obligations -- TTM: $44,938', 'Net cash provided by (used in) operating activities: $25,971']
Similarity Threshold: 0.03

Comparing: 
  Sentence: 'amazon.com, inc.'
  Target:   'operating cash flow -- trailing twelve months (ttm): $112,706'
  Similarity: 0.15584415584415584
Fuzzy match found! Phrase: 'Operating cash flow -- trailing twelve months (TTM): $112,706', Sentence: 'AMAZON.COM, INC.', Similarity: 0.15584415584415584
Comparing: 
  Sentence: 'supplemental financial information and business metrics (in millions, except per share data) (unaudited) q2 2023 q3 2023 q4 2023 q1 2024 q2 2024 q3 2024 y/y % change cash flows and shares operating cash flow -- trailing twelve months (ttm) $ 61,841 $ 71,654 $ 84,946 $ 99,147 $ 107,952 $ 112,706 57 % operating cash flow -- ttm y/y growth 74 % 81 % 82 % 82 % 75 % 57 % n/a purchases of property and equipment, net of proceeds from sales and incentives -- ttm $ 53,963 $ 50,220 $ 48,133 $ 48,998 $ 54,979 $ 64,959 29 % principal repayments of finance leases -- ttm $ 5,705 $ 5,245 $ 4,384 $ 3,774 $ 3,092 $ 2,489 (53) % principal repayments of financing obligations -- ttm $ 244 $ 260 $ 271 $ 304 $ 306 $ 320 23 % equipment acquired under finance leases -- ttm (1) $ 269 $ 239 $ 310 $ 306 $ 425 $ 492 106 % principal repayments of all other finance leases -- ttm (2) $ 631 $ 694 $ 683 $ 761 $ 794 $ 785 13 % free cash flow -- ttm (3) $ 7,878 $ 21,434 $ 36,813 $ 50,149 $ 52,973 $ 47,747 123 % free cash flow less principal repayments of finance leases and financing obligations -- ttm (4) $ 1,929 $ 15,929 $ 32,158 $ 46,071 $ 49,575 $ 44,938 182 % free cash flow less equipment finance leases and principal repayments of all other finance leases and financing obligations -- ttm (5) $ 6,734 $ 20,241 $ 35,549 $ 48,778 $ 51,448 $ 46,150 128 % common shares and stock-based awards outstanding 10,794 10,792 10,788 10,788 10,871 10,872 1 % common shares outstanding 10,313 10,330 10,383 10,403 10,490 10,511 2 % stock-based awards outstanding 481 462 406 385 381 361 (22) % stock-based awards outstanding -- % of common shares outstanding 4.7 % 4.5 % 3.9 % 3.7 % 3.6 % 3.4 % n/a results of operations worldwide (ww) net sales $ 134,383 $ 143,083 $ 169,961 $ 143,313 $ 147,977 $ 158,877 11 % ww net sales -- y/y growth, excluding f/x 11 % 11 % 13 % 13 % 11 % 11 % n/a ww net sales -- ttm $ 538,046 $ 554,028 $ 574,785 $ 590,740 $ 604,334 $ 620,128 12 % ww net sales -- ttm y/y growth, excluding f/x 13 % 12 % 12 % 12 % 12 % 12 % n/a operating income $ 7,681 $ 11,188 $ 13,209 $ 15,307 $ 14,672 $ 17,411 56 % f/x impact -- favorable $ 104 $ 132 $ 85 $ 72 $ 29 $ 16 n/a operating income -- y/y growth, excluding f/x 128 % 338 % 379 % 219 % 91 % 55 % n/a operating margin -- % of ww net sales 5.7 % 7.8 % 7.8 % 10.7 % 9.9 % 11.0 % n/a operating income -- ttm $ 17,717 $ 26,380 $ 36,852 $ 47,385 $ 54,376 $ 60,599 130 % operating income -- ttm y/y growth, excluding f/x 10 % 99 % 197 % 252 % 205 % 129 % n/a operating margin -- ttm % of ww net sales 3.3 % 4.8 % 6.4 % 8.0 % 9.0 % 9.8 % n/a net income $ 6,750 $ 9,879 $ 10,624 $ 10,431 $ 13,485 $ 15,328 55 % net income per diluted share $ 0.65 $ 0.94 $ 1.00 $ 0.98 $ 1.26 $ 1.43 53 % net income -- ttm $ 13,072 $ 20,079 $ 30,425 $ 37,684 $ 44,419 $ 49,868 148 % net income per diluted share -- ttm $ 1.26 $ 1.93 $ 2.90 $ 3.56 $ 4.18 $ 4.67 142 % ______________________________ (1) for the twelve months ended september 30, 2023 and 2024, this amount relates to equipment included in “property and equipment acquired under finance leases, net of remeasurements and modifications” of $748 million and $620 million.'
  Target:   'operating cash flow -- trailing twelve months (ttm): $112,706'
  Similarity: 0.037336652146857496
Fuzzy match found! Phrase: 'Operating cash flow -- trailing twelve months (TTM): $112,706', Sentence: 'Supplemental Financial Information and Business Metrics
(in millions, except per share data)
(unaudited)
Q2 2023
Q3 2023
Q4 2023
Q1 2024
Q2 2024
Q3 2024
Y/Y %
Change
Cash Flows and Shares
Operating cash flow -- trailing twelve months (TTM)
$ 
61,841 
$ 
71,654 
$ 
84,946 
$ 
99,147 
$ 107,952 
$ 112,706 
 57 %
Operating cash flow -- TTM Y/Y growth
 74 %
 81 %
 82 %
 82 %
 75 %
 57 %
N/A
Purchases of property and equipment, net of proceeds from sales and 
incentives -- TTM
$ 
53,963 
$ 
50,220 
$ 
48,133 
$ 
48,998 
$ 
54,979 
$ 
64,959 
 29 %
Principal repayments of finance leases -- TTM
$ 
5,705 
$ 
5,245 
$ 
4,384 
$ 
3,774 
$ 
3,092 
$ 
2,489 
 (53) %
Principal repayments of financing obligations -- TTM
$ 
244 
$ 
260 
$ 
271 
$ 
304 
$ 
306 
$ 
320 
 23 %
Equipment acquired under finance leases -- TTM (1)
$ 
269 
$ 
239 
$ 
310 
$ 
306 
$ 
425 
$ 
492 
 106 %
Principal repayments of all other finance leases -- TTM (2)
$ 
631 
$ 
694 
$ 
683 
$ 
761 
$ 
794 
$ 
785 
 13 %
Free cash flow -- TTM (3)
$ 
7,878 
$ 
21,434 
$ 
36,813 
$ 
50,149 
$ 
52,973 
$ 
47,747 
 123 %
Free cash flow less principal repayments of finance leases and 
financing obligations -- TTM (4)
$ 
1,929 
$ 
15,929 
$ 
32,158 
$ 
46,071 
$ 
49,575 
$ 
44,938 
 182 %
Free cash flow less equipment finance leases and principal repayments 
of all other finance leases and financing obligations -- TTM (5)
$ 
6,734 
$ 
20,241 
$ 
35,549 
$ 
48,778 
$ 
51,448 
$ 
46,150 
 128 %
Common shares and stock-based awards outstanding
 
10,794 
 
10,792 
 
10,788 
 
10,788 
 
10,871 
 
10,872 
 1 %
Common shares outstanding
 
10,313 
 
10,330 
 
10,383 
 
10,403 
 
10,490 
 
10,511 
 2 %
Stock-based awards outstanding
 
481 
 
462 
 
406 
 
385 
 
381 
 
361 
 (22) %
Stock-based awards outstanding -- % of common shares outstanding
 4.7 %
 4.5 %
 3.9 %
 3.7 %
 3.6 %
 3.4 %
N/A
Results of Operations
Worldwide (WW) net sales
$ 134,383 
$ 143,083 
$ 169,961 
$ 143,313 
$ 147,977 
$ 158,877 
 11 %
WW net sales -- Y/Y growth, excluding F/X
 11 %
 11 %
 13 %
 13 %
 11 %
 11 %
N/A
WW net sales -- TTM
$ 538,046 
$ 554,028 
$ 574,785 
$ 590,740 
$ 604,334 
$ 620,128 
 12 %
WW net sales -- TTM Y/Y growth, excluding F/X
 13 %
 12 %
 12 %
 12 %
 12 %
 12 %
N/A
Operating income
$ 
7,681 
$ 
11,188 
$ 
13,209 
$ 
15,307 
$ 
14,672 
$ 
17,411 
 56 %
F/X impact -- favorable
$ 
104 
$ 
132 
$ 
85 
$ 
72 
$ 
29 
$ 
16 
N/A
Operating income -- Y/Y growth, excluding F/X
 128 %
 338 %
 379 %
 219 %
 91 %
 55 %
N/A
Operating margin -- % of WW net sales
 5.7 %
 7.8 %
 7.8 %
 10.7 %
 9.9 %
 11.0 %
N/A
Operating income -- TTM
$ 
17,717 
$ 
26,380 
$ 
36,852 
$ 
47,385 
$ 
54,376 
$ 
60,599 
 130 %
Operating income -- TTM Y/Y growth, excluding F/X
 10 %
 99 %
 197 %
 252 %
 205 %
 129 %
N/A
Operating margin -- TTM % of WW net sales
 3.3 %
 4.8 %
 6.4 %
 8.0 %
 9.0 %
 9.8 %
N/A
Net income
$ 
6,750 
$ 
9,879 
$ 
10,624 
$ 
10,431 
$ 
13,485 
$ 
15,328 
 55 %
Net income per diluted share
$ 
0.65 
$ 
0.94 
$ 
1.00 
$ 
0.98 
$ 
1.26 
$ 
1.43 
 53 %
Net income -- TTM
$ 
13,072 
$ 
20,079 
$ 
30,425 
$ 
37,684 
$ 
44,419 
$ 
49,868 
 148 %
Net income per diluted share -- TTM
$ 
1.26 
$ 
1.93 
$ 
2.90 
$ 
3.56 
$ 
4.18 
$ 
4.67 
 142 %
______________________________
(1)
For the twelve months ended September 30, 2023 and 2024, this amount relates to equipment included in “Property and equipment acquired under finance leases, net of 
remeasurements and modifications” of $748 million and $620 million.', Similarity: 0.037336652146857496
Comparing: 
  Sentence: '(2) for the twelve months ended september 30, 2023 and 2024, this amount relates to property included in “principal repayments of finance leases” of $5,245 million and $2,489 million.'
  Target:   'operating cash flow -- trailing twelve months (ttm): $112,706'
  Similarity: 0.21311475409836064
Fuzzy match found! Phrase: 'Operating cash flow -- trailing twelve months (TTM): $112,706', Sentence: '(2)
For the twelve months ended September 30, 2023 and 2024, this amount relates to property included in “Principal repayments of finance leases” of $5,245 million and 
$2,489 million.', Similarity: 0.21311475409836064
Comparing: 
  Sentence: '(3) free cash flow is cash flow from operations reduced by “purchases of property and equipment, net of proceeds from sales and incentives.” (4) free cash flow less principal repayments of finance leases and financing obligations is free cash flow reduced by “principal repayments of finance leases” and “principal repayments of financing obligations.” (5) free cash flow less equipment finance leases and principal repayments of all other finance leases and financing obligations is free cash flow reduced by equipment acquired under finance leases, which is included in “property and equipment acquired under finance leases, net of remeasurements and modifications,” principal repayments of all other finance lease liabilities, which is included in “principal repayments of finance leases,” and “principal repayments of financing obligations.”'
  Target:   'operating cash flow -- trailing twelve months (ttm): $112,706'
  Similarity: 0.0706401766004415
Fuzzy match found! Phrase: 'Operating cash flow -- trailing twelve months (TTM): $112,706', Sentence: '(3)
Free cash flow is cash flow from operations reduced by “Purchases of property and equipment, net of proceeds from sales and incentives.”
(4)
Free cash flow less principal repayments of finance leases and financing obligations is free cash flow reduced by “Principal repayments of finance leases” and “Principal 
repayments of financing obligations.”
(5)
Free cash flow less equipment finance leases and principal repayments of all other finance leases and financing obligations is free cash flow reduced by equipment acquired 
under finance leases, which is included in “Property and equipment acquired under finance leases, net of remeasurements and modifications,” principal repayments of all 
other finance lease liabilities, which is included in “Principal repayments of finance leases,” and “Principal repayments of financing obligations.”', Similarity: 0.0706401766004415
Comparing: 
  Sentence: 'amazon.com, inc.'
  Target:   'free cash flow -- ttm: $47,747'
  Similarity: 0.13043478260869565
Fuzzy match found! Phrase: 'Free cash flow -- TTM: $47,747', Sentence: 'AMAZON.COM, INC.', Similarity: 0.13043478260869565
Comparing: 
  Sentence: 'supplemental financial information and business metrics (in millions, except per share data) (unaudited) q2 2023 q3 2023 q4 2023 q1 2024 q2 2024 q3 2024 y/y % change cash flows and shares operating cash flow -- trailing twelve months (ttm) $ 61,841 $ 71,654 $ 84,946 $ 99,147 $ 107,952 $ 112,706 57 % operating cash flow -- ttm y/y growth 74 % 81 % 82 % 82 % 75 % 57 % n/a purchases of property and equipment, net of proceeds from sales and incentives -- ttm $ 53,963 $ 50,220 $ 48,133 $ 48,998 $ 54,979 $ 64,959 29 % principal repayments of finance leases -- ttm $ 5,705 $ 5,245 $ 4,384 $ 3,774 $ 3,092 $ 2,489 (53) % principal repayments of financing obligations -- ttm $ 244 $ 260 $ 271 $ 304 $ 306 $ 320 23 % equipment acquired under finance leases -- ttm (1) $ 269 $ 239 $ 310 $ 306 $ 425 $ 492 106 % principal repayments of all other finance leases -- ttm (2) $ 631 $ 694 $ 683 $ 761 $ 794 $ 785 13 % free cash flow -- ttm (3) $ 7,878 $ 21,434 $ 36,813 $ 50,149 $ 52,973 $ 47,747 123 % free cash flow less principal repayments of finance leases and financing obligations -- ttm (4) $ 1,929 $ 15,929 $ 32,158 $ 46,071 $ 49,575 $ 44,938 182 % free cash flow less equipment finance leases and principal repayments of all other finance leases and financing obligations -- ttm (5) $ 6,734 $ 20,241 $ 35,549 $ 48,778 $ 51,448 $ 46,150 128 % common shares and stock-based awards outstanding 10,794 10,792 10,788 10,788 10,871 10,872 1 % common shares outstanding 10,313 10,330 10,383 10,403 10,490 10,511 2 % stock-based awards outstanding 481 462 406 385 381 361 (22) % stock-based awards outstanding -- % of common shares outstanding 4.7 % 4.5 % 3.9 % 3.7 % 3.6 % 3.4 % n/a results of operations worldwide (ww) net sales $ 134,383 $ 143,083 $ 169,961 $ 143,313 $ 147,977 $ 158,877 11 % ww net sales -- y/y growth, excluding f/x 11 % 11 % 13 % 13 % 11 % 11 % n/a ww net sales -- ttm $ 538,046 $ 554,028 $ 574,785 $ 590,740 $ 604,334 $ 620,128 12 % ww net sales -- ttm y/y growth, excluding f/x 13 % 12 % 12 % 12 % 12 % 12 % n/a operating income $ 7,681 $ 11,188 $ 13,209 $ 15,307 $ 14,672 $ 17,411 56 % f/x impact -- favorable $ 104 $ 132 $ 85 $ 72 $ 29 $ 16 n/a operating income -- y/y growth, excluding f/x 128 % 338 % 379 % 219 % 91 % 55 % n/a operating margin -- % of ww net sales 5.7 % 7.8 % 7.8 % 10.7 % 9.9 % 11.0 % n/a operating income -- ttm $ 17,717 $ 26,380 $ 36,852 $ 47,385 $ 54,376 $ 60,599 130 % operating income -- ttm y/y growth, excluding f/x 10 % 99 % 197 % 252 % 205 % 129 % n/a operating margin -- ttm % of ww net sales 3.3 % 4.8 % 6.4 % 8.0 % 9.0 % 9.8 % n/a net income $ 6,750 $ 9,879 $ 10,624 $ 10,431 $ 13,485 $ 15,328 55 % net income per diluted share $ 0.65 $ 0.94 $ 1.00 $ 0.98 $ 1.26 $ 1.43 53 % net income -- ttm $ 13,072 $ 20,079 $ 30,425 $ 37,684 $ 44,419 $ 49,868 148 % net income per diluted share -- ttm $ 1.26 $ 1.93 $ 2.90 $ 3.56 $ 4.18 $ 4.67 142 % ______________________________ (1) for the twelve months ended september 30, 2023 and 2024, this amount relates to equipment included in “property and equipment acquired under finance leases, net of remeasurements and modifications” of $748 million and $620 million.'
  Target:   'free cash flow -- ttm: $47,747'
  Similarity: 0.018221803330191643
Comparing: 
  Sentence: '(2) for the twelve months ended september 30, 2023 and 2024, this amount relates to property included in “principal repayments of finance leases” of $5,245 million and $2,489 million.'
  Target:   'free cash flow -- ttm: $47,747'
  Similarity: 0.10328638497652583
Fuzzy match found! Phrase: 'Free cash flow -- TTM: $47,747', Sentence: '(2)
For the twelve months ended September 30, 2023 and 2024, this amount relates to property included in “Principal repayments of finance leases” of $5,245 million and 
$2,489 million.', Similarity: 0.10328638497652583
Comparing: 
  Sentence: '(3) free cash flow is cash flow from operations reduced by “purchases of property and equipment, net of proceeds from sales and incentives.” (4) free cash flow less principal repayments of finance leases and financing obligations is free cash flow reduced by “principal repayments of finance leases” and “principal repayments of financing obligations.” (5) free cash flow less equipment finance leases and principal repayments of all other finance leases and financing obligations is free cash flow reduced by equipment acquired under finance leases, which is included in “property and equipment acquired under finance leases, net of remeasurements and modifications,” principal repayments of all other finance lease liabilities, which is included in “principal repayments of finance leases,” and “principal repayments of financing obligations.”'
  Target:   'free cash flow -- ttm: $47,747'
  Similarity: 0.04342857142857143
Fuzzy match found! Phrase: 'Free cash flow -- TTM: $47,747', Sentence: '(3)
Free cash flow is cash flow from operations reduced by “Purchases of property and equipment, net of proceeds from sales and incentives.”
(4)
Free cash flow less principal repayments of finance leases and financing obligations is free cash flow reduced by “Principal repayments of finance leases” and “Principal 
repayments of financing obligations.”
(5)
Free cash flow less equipment finance leases and principal repayments of all other finance leases and financing obligations is free cash flow reduced by equipment acquired 
under finance leases, which is included in “Property and equipment acquired under finance leases, net of remeasurements and modifications,” principal repayments of all 
other finance lease liabilities, which is included in “Principal repayments of finance leases,” and “Principal repayments of financing obligations.”', Similarity: 0.04342857142857143
Comparing: 
  Sentence: 'amazon.com, inc.'
  Target:   'free cash flow less principal repayments of finance leases and financing obligations -- ttm: $44,938'
  Similarity: 0.10344827586206896
Fuzzy match found! Phrase: 'Free cash flow less principal repayments of finance leases and financing obligations -- TTM: $44,938', Sentence: 'AMAZON.COM, INC.', Similarity: 0.10344827586206896
Comparing: 
  Sentence: 'supplemental financial information and business metrics (in millions, except per share data) (unaudited) q2 2023 q3 2023 q4 2023 q1 2024 q2 2024 q3 2024 y/y % change cash flows and shares operating cash flow -- trailing twelve months (ttm) $ 61,841 $ 71,654 $ 84,946 $ 99,147 $ 107,952 $ 112,706 57 % operating cash flow -- ttm y/y growth 74 % 81 % 82 % 82 % 75 % 57 % n/a purchases of property and equipment, net of proceeds from sales and incentives -- ttm $ 53,963 $ 50,220 $ 48,133 $ 48,998 $ 54,979 $ 64,959 29 % principal repayments of finance leases -- ttm $ 5,705 $ 5,245 $ 4,384 $ 3,774 $ 3,092 $ 2,489 (53) % principal repayments of financing obligations -- ttm $ 244 $ 260 $ 271 $ 304 $ 306 $ 320 23 % equipment acquired under finance leases -- ttm (1) $ 269 $ 239 $ 310 $ 306 $ 425 $ 492 106 % principal repayments of all other finance leases -- ttm (2) $ 631 $ 694 $ 683 $ 761 $ 794 $ 785 13 % free cash flow -- ttm (3) $ 7,878 $ 21,434 $ 36,813 $ 50,149 $ 52,973 $ 47,747 123 % free cash flow less principal repayments of finance leases and financing obligations -- ttm (4) $ 1,929 $ 15,929 $ 32,158 $ 46,071 $ 49,575 $ 44,938 182 % free cash flow less equipment finance leases and principal repayments of all other finance leases and financing obligations -- ttm (5) $ 6,734 $ 20,241 $ 35,549 $ 48,778 $ 51,448 $ 46,150 128 % common shares and stock-based awards outstanding 10,794 10,792 10,788 10,788 10,871 10,872 1 % common shares outstanding 10,313 10,330 10,383 10,403 10,490 10,511 2 % stock-based awards outstanding 481 462 406 385 381 361 (22) % stock-based awards outstanding -- % of common shares outstanding 4.7 % 4.5 % 3.9 % 3.7 % 3.6 % 3.4 % n/a results of operations worldwide (ww) net sales $ 134,383 $ 143,083 $ 169,961 $ 143,313 $ 147,977 $ 158,877 11 % ww net sales -- y/y growth, excluding f/x 11 % 11 % 13 % 13 % 11 % 11 % n/a ww net sales -- ttm $ 538,046 $ 554,028 $ 574,785 $ 590,740 $ 604,334 $ 620,128 12 % ww net sales -- ttm y/y growth, excluding f/x 13 % 12 % 12 % 12 % 12 % 12 % n/a operating income $ 7,681 $ 11,188 $ 13,209 $ 15,307 $ 14,672 $ 17,411 56 % f/x impact -- favorable $ 104 $ 132 $ 85 $ 72 $ 29 $ 16 n/a operating income -- y/y growth, excluding f/x 128 % 338 % 379 % 219 % 91 % 55 % n/a operating margin -- % of ww net sales 5.7 % 7.8 % 7.8 % 10.7 % 9.9 % 11.0 % n/a operating income -- ttm $ 17,717 $ 26,380 $ 36,852 $ 47,385 $ 54,376 $ 60,599 130 % operating income -- ttm y/y growth, excluding f/x 10 % 99 % 197 % 252 % 205 % 129 % n/a operating margin -- ttm % of ww net sales 3.3 % 4.8 % 6.4 % 8.0 % 9.0 % 9.8 % n/a net income $ 6,750 $ 9,879 $ 10,624 $ 10,431 $ 13,485 $ 15,328 55 % net income per diluted share $ 0.65 $ 0.94 $ 1.00 $ 0.98 $ 1.26 $ 1.43 53 % net income -- ttm $ 13,072 $ 20,079 $ 30,425 $ 37,684 $ 44,419 $ 49,868 148 % net income per diluted share -- ttm $ 1.26 $ 1.93 $ 2.90 $ 3.56 $ 4.18 $ 4.67 142 % ______________________________ (1) for the twelve months ended september 30, 2023 and 2024, this amount relates to equipment included in “property and equipment acquired under finance leases, net of remeasurements and modifications” of $748 million and $620 million.'
  Target:   'free cash flow less principal repayments of finance leases and financing obligations -- ttm: $44,938'
  Similarity: 0.06086689209960037
Fuzzy match found! Phrase: 'Free cash flow less principal repayments of finance leases and financing obligations -- TTM: $44,938', Sentence: 'Supplemental Financial Information and Business Metrics
(in millions, except per share data)
(unaudited)
Q2 2023
Q3 2023
Q4 2023
Q1 2024
Q2 2024
Q3 2024
Y/Y %
Change
Cash Flows and Shares
Operating cash flow -- trailing twelve months (TTM)
$ 
61,841 
$ 
71,654 
$ 
84,946 
$ 
99,147 
$ 107,952 
$ 112,706 
 57 %
Operating cash flow -- TTM Y/Y growth
 74 %
 81 %
 82 %
 82 %
 75 %
 57 %
N/A
Purchases of property and equipment, net of proceeds from sales and 
incentives -- TTM
$ 
53,963 
$ 
50,220 
$ 
48,133 
$ 
48,998 
$ 
54,979 
$ 
64,959 
 29 %
Principal repayments of finance leases -- TTM
$ 
5,705 
$ 
5,245 
$ 
4,384 
$ 
3,774 
$ 
3,092 
$ 
2,489 
 (53) %
Principal repayments of financing obligations -- TTM
$ 
244 
$ 
260 
$ 
271 
$ 
304 
$ 
306 
$ 
320 
 23 %
Equipment acquired under finance leases -- TTM (1)
$ 
269 
$ 
239 
$ 
310 
$ 
306 
$ 
425 
$ 
492 
 106 %
Principal repayments of all other finance leases -- TTM (2)
$ 
631 
$ 
694 
$ 
683 
$ 
761 
$ 
794 
$ 
785 
 13 %
Free cash flow -- TTM (3)
$ 
7,878 
$ 
21,434 
$ 
36,813 
$ 
50,149 
$ 
52,973 
$ 
47,747 
 123 %
Free cash flow less principal repayments of finance leases and 
financing obligations -- TTM (4)
$ 
1,929 
$ 
15,929 
$ 
32,158 
$ 
46,071 
$ 
49,575 
$ 
44,938 
 182 %
Free cash flow less equipment finance leases and principal repayments 
of all other finance leases and financing obligations -- TTM (5)
$ 
6,734 
$ 
20,241 
$ 
35,549 
$ 
48,778 
$ 
51,448 
$ 
46,150 
 128 %
Common shares and stock-based awards outstanding
 
10,794 
 
10,792 
 
10,788 
 
10,788 
 
10,871 
 
10,872 
 1 %
Common shares outstanding
 
10,313 
 
10,330 
 
10,383 
 
10,403 
 
10,490 
 
10,511 
 2 %
Stock-based awards outstanding
 
481 
 
462 
 
406 
 
385 
 
381 
 
361 
 (22) %
Stock-based awards outstanding -- % of common shares outstanding
 4.7 %
 4.5 %
 3.9 %
 3.7 %
 3.6 %
 3.4 %
N/A
Results of Operations
Worldwide (WW) net sales
$ 134,383 
$ 143,083 
$ 169,961 
$ 143,313 
$ 147,977 
$ 158,877 
 11 %
WW net sales -- Y/Y growth, excluding F/X
 11 %
 11 %
 13 %
 13 %
 11 %
 11 %
N/A
WW net sales -- TTM
$ 538,046 
$ 554,028 
$ 574,785 
$ 590,740 
$ 604,334 
$ 620,128 
 12 %
WW net sales -- TTM Y/Y growth, excluding F/X
 13 %
 12 %
 12 %
 12 %
 12 %
 12 %
N/A
Operating income
$ 
7,681 
$ 
11,188 
$ 
13,209 
$ 
15,307 
$ 
14,672 
$ 
17,411 
 56 %
F/X impact -- favorable
$ 
104 
$ 
132 
$ 
85 
$ 
72 
$ 
29 
$ 
16 
N/A
Operating income -- Y/Y growth, excluding F/X
 128 %
 338 %
 379 %
 219 %
 91 %
 55 %
N/A
Operating margin -- % of WW net sales
 5.7 %
 7.8 %
 7.8 %
 10.7 %
 9.9 %
 11.0 %
N/A
Operating income -- TTM
$ 
17,717 
$ 
26,380 
$ 
36,852 
$ 
47,385 
$ 
54,376 
$ 
60,599 
 130 %
Operating income -- TTM Y/Y growth, excluding F/X
 10 %
 99 %
 197 %
 252 %
 205 %
 129 %
N/A
Operating margin -- TTM % of WW net sales
 3.3 %
 4.8 %
 6.4 %
 8.0 %
 9.0 %
 9.8 %
N/A
Net income
$ 
6,750 
$ 
9,879 
$ 
10,624 
$ 
10,431 
$ 
13,485 
$ 
15,328 
 55 %
Net income per diluted share
$ 
0.65 
$ 
0.94 
$ 
1.00 
$ 
0.98 
$ 
1.26 
$ 
1.43 
 53 %
Net income -- TTM
$ 
13,072 
$ 
20,079 
$ 
30,425 
$ 
37,684 
$ 
44,419 
$ 
49,868 
 148 %
Net income per diluted share -- TTM
$ 
1.26 
$ 
1.93 
$ 
2.90 
$ 
3.56 
$ 
4.18 
$ 
4.67 
 142 %
______________________________
(1)
For the twelve months ended September 30, 2023 and 2024, this amount relates to equipment included in “Property and equipment acquired under finance leases, net of 
remeasurements and modifications” of $748 million and $620 million.', Similarity: 0.06086689209960037
Comparing: 
  Sentence: '(2) for the twelve months ended september 30, 2023 and 2024, this amount relates to property included in “principal repayments of finance leases” of $5,245 million and $2,489 million.'
  Target:   'free cash flow less principal repayments of finance leases and financing obligations -- ttm: $44,938'
  Similarity: 0.38869257950530034
Fuzzy match found! Phrase: 'Free cash flow less principal repayments of finance leases and financing obligations -- TTM: $44,938', Sentence: '(2)
For the twelve months ended September 30, 2023 and 2024, this amount relates to property included in “Principal repayments of finance leases” of $5,245 million and 
$2,489 million.', Similarity: 0.38869257950530034
Comparing: 
  Sentence: '(3) free cash flow is cash flow from operations reduced by “purchases of property and equipment, net of proceeds from sales and incentives.” (4) free cash flow less principal repayments of finance leases and financing obligations is free cash flow reduced by “principal repayments of finance leases” and “principal repayments of financing obligations.” (5) free cash flow less equipment finance leases and principal repayments of all other finance leases and financing obligations is free cash flow reduced by equipment acquired under finance leases, which is included in “property and equipment acquired under finance leases, net of remeasurements and modifications,” principal repayments of all other finance lease liabilities, which is included in “principal repayments of finance leases,” and “principal repayments of financing obligations.”'
  Target:   'free cash flow less principal repayments of finance leases and financing obligations -- ttm: $44,938'
  Similarity: 0.18624338624338624
Fuzzy match found! Phrase: 'Free cash flow less principal repayments of finance leases and financing obligations -- TTM: $44,938', Sentence: '(3)
Free cash flow is cash flow from operations reduced by “Purchases of property and equipment, net of proceeds from sales and incentives.”
(4)
Free cash flow less principal repayments of finance leases and financing obligations is free cash flow reduced by “Principal repayments of finance leases” and “Principal 
repayments of financing obligations.”
(5)
Free cash flow less equipment finance leases and principal repayments of all other finance leases and financing obligations is free cash flow reduced by equipment acquired 
under finance leases, which is included in “Property and equipment acquired under finance leases, net of remeasurements and modifications,” principal repayments of all 
other finance lease liabilities, which is included in “Principal repayments of finance leases,” and “Principal repayments of financing obligations.”', Similarity: 0.18624338624338624
Comparing: 
  Sentence: 'amazon.com, inc.'
  Target:   'net cash provided by (used in) operating activities: $25,971'
  Similarity: 0.15789473684210525
Fuzzy match found! Phrase: 'Net cash provided by (used in) operating activities: $25,971', Sentence: 'AMAZON.COM, INC.', Similarity: 0.15789473684210525
Comparing: 
  Sentence: 'supplemental financial information and business metrics (in millions, except per share data) (unaudited) q2 2023 q3 2023 q4 2023 q1 2024 q2 2024 q3 2024 y/y % change cash flows and shares operating cash flow -- trailing twelve months (ttm) $ 61,841 $ 71,654 $ 84,946 $ 99,147 $ 107,952 $ 112,706 57 % operating cash flow -- ttm y/y growth 74 % 81 % 82 % 82 % 75 % 57 % n/a purchases of property and equipment, net of proceeds from sales and incentives -- ttm $ 53,963 $ 50,220 $ 48,133 $ 48,998 $ 54,979 $ 64,959 29 % principal repayments of finance leases -- ttm $ 5,705 $ 5,245 $ 4,384 $ 3,774 $ 3,092 $ 2,489 (53) % principal repayments of financing obligations -- ttm $ 244 $ 260 $ 271 $ 304 $ 306 $ 320 23 % equipment acquired under finance leases -- ttm (1) $ 269 $ 239 $ 310 $ 306 $ 425 $ 492 106 % principal repayments of all other finance leases -- ttm (2) $ 631 $ 694 $ 683 $ 761 $ 794 $ 785 13 % free cash flow -- ttm (3) $ 7,878 $ 21,434 $ 36,813 $ 50,149 $ 52,973 $ 47,747 123 % free cash flow less principal repayments of finance leases and financing obligations -- ttm (4) $ 1,929 $ 15,929 $ 32,158 $ 46,071 $ 49,575 $ 44,938 182 % free cash flow less equipment finance leases and principal repayments of all other finance leases and financing obligations -- ttm (5) $ 6,734 $ 20,241 $ 35,549 $ 48,778 $ 51,448 $ 46,150 128 % common shares and stock-based awards outstanding 10,794 10,792 10,788 10,788 10,871 10,872 1 % common shares outstanding 10,313 10,330 10,383 10,403 10,490 10,511 2 % stock-based awards outstanding 481 462 406 385 381 361 (22) % stock-based awards outstanding -- % of common shares outstanding 4.7 % 4.5 % 3.9 % 3.7 % 3.6 % 3.4 % n/a results of operations worldwide (ww) net sales $ 134,383 $ 143,083 $ 169,961 $ 143,313 $ 147,977 $ 158,877 11 % ww net sales -- y/y growth, excluding f/x 11 % 11 % 13 % 13 % 11 % 11 % n/a ww net sales -- ttm $ 538,046 $ 554,028 $ 574,785 $ 590,740 $ 604,334 $ 620,128 12 % ww net sales -- ttm y/y growth, excluding f/x 13 % 12 % 12 % 12 % 12 % 12 % n/a operating income $ 7,681 $ 11,188 $ 13,209 $ 15,307 $ 14,672 $ 17,411 56 % f/x impact -- favorable $ 104 $ 132 $ 85 $ 72 $ 29 $ 16 n/a operating income -- y/y growth, excluding f/x 128 % 338 % 379 % 219 % 91 % 55 % n/a operating margin -- % of ww net sales 5.7 % 7.8 % 7.8 % 10.7 % 9.9 % 11.0 % n/a operating income -- ttm $ 17,717 $ 26,380 $ 36,852 $ 47,385 $ 54,376 $ 60,599 130 % operating income -- ttm y/y growth, excluding f/x 10 % 99 % 197 % 252 % 205 % 129 % n/a operating margin -- ttm % of ww net sales 3.3 % 4.8 % 6.4 % 8.0 % 9.0 % 9.8 % n/a net income $ 6,750 $ 9,879 $ 10,624 $ 10,431 $ 13,485 $ 15,328 55 % net income per diluted share $ 0.65 $ 0.94 $ 1.00 $ 0.98 $ 1.26 $ 1.43 53 % net income -- ttm $ 13,072 $ 20,079 $ 30,425 $ 37,684 $ 44,419 $ 49,868 148 % net income per diluted share -- ttm $ 1.26 $ 1.93 $ 2.90 $ 3.56 $ 4.18 $ 4.67 142 % ______________________________ (1) for the twelve months ended september 30, 2023 and 2024, this amount relates to equipment included in “property and equipment acquired under finance leases, net of remeasurements and modifications” of $748 million and $620 million.'
  Target:   'net cash provided by (used in) operating activities: $25,971'
  Similarity: 0.023653906006847185
Comparing: 
  Sentence: '(2) for the twelve months ended september 30, 2023 and 2024, this amount relates to property included in “principal repayments of finance leases” of $5,245 million and $2,489 million.'
  Target:   'net cash provided by (used in) operating activities: $25,971'
  Similarity: 0.26337448559670784
Fuzzy match found! Phrase: 'Net cash provided by (used in) operating activities: $25,971', Sentence: '(2)
For the twelve months ended September 30, 2023 and 2024, this amount relates to property included in “Principal repayments of finance leases” of $5,245 million and 
$2,489 million.', Similarity: 0.26337448559670784
Comparing: 
  Sentence: '(3) free cash flow is cash flow from operations reduced by “purchases of property and equipment, net of proceeds from sales and incentives.” (4) free cash flow less principal repayments of finance leases and financing obligations is free cash flow reduced by “principal repayments of finance leases” and “principal repayments of financing obligations.” (5) free cash flow less equipment finance leases and principal repayments of all other finance leases and financing obligations is free cash flow reduced by equipment acquired under finance leases, which is included in “property and equipment acquired under finance leases, net of remeasurements and modifications,” principal repayments of all other finance lease liabilities, which is included in “principal repayments of finance leases,” and “principal repayments of financing obligations.”'
  Target:   'net cash provided by (used in) operating activities: $25,971'
  Similarity: 0.06850828729281767
Fuzzy match found! Phrase: 'Net cash provided by (used in) operating activities: $25,971', Sentence: '(3)
Free cash flow is cash flow from operations reduced by “Purchases of property and equipment, net of proceeds from sales and incentives.”
(4)
Free cash flow less principal repayments of finance leases and financing obligations is free cash flow reduced by “Principal repayments of finance leases” and “Principal 
repayments of financing obligations.”
(5)
Free cash flow less equipment finance leases and principal repayments of all other finance leases and financing obligations is free cash flow reduced by equipment acquired 
under finance leases, which is included in “Property and equipment acquired under finance leases, net of remeasurements and modifications,” principal repayments of all 
other finance lease liabilities, which is included in “Principal repayments of finance leases,” and “Principal repayments of financing obligations.”', Similarity: 0.06850828729281767

Highlight Summary:
Total Highlights: 808
Highlighted: True

--- Sentence Matching Process ---
Target Page: 4
Search Phrases: ['Net cash provided by (used in) operating activities: $25,971']
Similarity Threshold: 0.03

Comparing: 
  Sentence: 'amazon.com, inc.'
  Target:   'net cash provided by (used in) operating activities: $25,971'
  Similarity: 0.15789473684210525
Fuzzy match found! Phrase: 'Net cash provided by (used in) operating activities: $25,971', Sentence: 'AMAZON.COM, INC.', Similarity: 0.15789473684210525
Comparing: 
  Sentence: 'consolidated statements of cash flows (in millions) (unaudited) three months ended september 30, nine months ended september 30, twelve months ended september 30, 2023 2024 2023 2024 2023 2024 cash, cash equivalents, and restricted cash, beginning of period $ 50,067 $ 71,673 $ 54,253 $ 73,890 $ 35,178 $ 50,081 operating activities: net income 9,879 15,328 19,801 39,244 20,079 49,868 adjustments to reconcile net income to net cash from operating activities: depreciation and amortization of property and equipment and capitalized content costs, operating lease assets, and other 12,131 13,442 34,843 37,164 47,528 50,984 stock-based compensation 5,829 5,333 17,704 17,016 23,310 23,335 non-operating expense (income), net (990) (141) (409) 2,498 3,036 2,159 deferred income taxes (1,196) (1,317) (4,412) (3,040) (7,779) (4,504) changes in operating assets and liabilities: inventories 808 (1,509) (1,194) (2,818) 1,986 (175) accounts receivable, net and other (3,584) (701) (901) 774 (5,641) (6,673) other assets (3,134) (4,537) (9,463) (10,293) (13,511) (13,095) accounts payable 2,820 (477) (5,415) (5,754) 4,437 5,134 accrued expenses and other (1,321) 129 (9,022) (6,946) (3,245) (352) unearned revenue (25) 421 949 2,396 1,454 6,025 net cash provided by (used in) operating activities 21,217 25,971 42,481 70,241 71,654 112,706 investing activities: purchases of property and equipment (12,479) (22,620) (38,141) (55,165) (54,733) (69,753) proceeds from property and equipment sales and incentives 1,181 1,342 3,361 3,559 4,513 4,794 acquisitions, net of cash acquired, non-marketable investments, and other (1,629) (622) (5,458) (4,547) (6,289) (4,928) sales and maturities of marketable securities 1,393 8,069 4,059 12,726 9,742 14,294 purchases of marketable securities (219) (3,068) (1,053) (13,472) (1,286) (13,907) net cash provided by (used in) investing activities (11,753) (16,899) (37,232) (56,899) (48,053) (69,500) financing activities: proceeds from short-term debt, and other 216 1,725 17,395 2,588 28,002 3,322 repayments of short-term debt, and other (8,095) (1,820) (19,339) (2,453) (35,136) (8,791) proceeds from long-term debt — — — — 8,235 — repayments of long-term debt — (2,183) (3,386) (6,682) (4,643) (6,972) principal repayments of finance leases (1,005) (402) (3,605) (1,710) (5,245) (2,489) principal repayments of financing obligations (64) (78) (198) (247) (260) (320) net cash provided by (used in) financing activities (8,948) (2,758) (9,133) (8,504) (9,047) (15,250) foreign currency effect on cash, cash equivalents, and restricted cash (502) 690 (288) (51) 349 640 net increase (decrease) in cash, cash equivalents, and restricted cash 14 7,004 (4,172) 4,787 14,903 28,596 cash, cash equivalents, and restricted cash, end of period $ 50,081 $ 78,677 $ 50,081 $ 78,677 $ 50,081 $ 78,677 supplemental cash flow information: cash paid for interest on debt, net of capitalized interest $ 465 $ 266 $ 1,821 $ 1,215 $ 2,450 $ 2,002 cash paid for operating leases 2,692 2,940 7,687 9,116 10,052 11,882 cash paid for interest on finance leases 76 71 234 217 318 291 cash paid for interest on financing obligations 50 47 150 161 205 207 cash paid for income taxes, net of refunds 2,628 2,004 6,982 8,162 8,677 12,359 assets acquired under operating leases 3,345 3,571 11,075 11,235 15,844 14,212 property and equipment acquired under finance leases, net of remeasurements and modifications 183 186 431 409 748 620 property and equipment recognized during the construction period of build-to-suit lease arrangements 93 21 308 89 618 138 property and equipment derecognized after the construction period of build-to-suit lease arrangements, with the associated leases recognized as operating 492 — 1,212 — 3,063 162'
  Target:   'net cash provided by (used in) operating activities: $25,971'
  Similarity: 0.03047819232790331
Fuzzy match found! Phrase: 'Net cash provided by (used in) operating activities: $25,971', Sentence: 'Consolidated Statements of Cash Flows
(in millions)
(unaudited) 
  
Three Months Ended
September 30,
Nine Months Ended
September 30,
Twelve Months Ended
September 30,
 
2023
2024
2023
2024
2023
2024
CASH, CASH EQUIVALENTS, AND RESTRICTED CASH, BEGINNING OF 
PERIOD
$ 
50,067 
$ 
71,673 
$ 
54,253 
$ 
73,890 
$ 
35,178 
$ 
50,081 
OPERATING ACTIVITIES:
Net income
 
9,879 
 
15,328 
 
19,801 
 
39,244 
 
20,079 
 
49,868 
Adjustments to reconcile net income to net cash from operating activities:
Depreciation and amortization of property and equipment and capitalized 
content costs, operating lease assets, and other
 
12,131 
 
13,442 
 
34,843 
 
37,164 
 
47,528 
 
50,984 
Stock-based compensation
 
5,829 
 
5,333 
 
17,704 
 
17,016 
 
23,310 
 
23,335 
Non-operating expense (income), net
 
(990)  
(141)  
(409)  
2,498 
 
3,036 
 
2,159 
Deferred income taxes
 
(1,196)  
(1,317)  
(4,412)  
(3,040)  
(7,779)  
(4,504) 
Changes in operating assets and liabilities:
Inventories
 
808 
 
(1,509)  
(1,194)  
(2,818)  
1,986 
 
(175) 
Accounts receivable, net and other
 
(3,584)  
(701)  
(901)  
774 
 
(5,641)  
(6,673) 
Other assets
 
(3,134)  
(4,537)  
(9,463)  
(10,293)  
(13,511)  
(13,095) 
Accounts payable
 
2,820 
 
(477)  
(5,415)  
(5,754)  
4,437 
 
5,134 
Accrued expenses and other
 
(1,321)  
129 
 
(9,022)  
(6,946)  
(3,245)  
(352) 
Unearned revenue
 
(25)  
421 
 
949 
 
2,396 
 
1,454 
 
6,025 
Net cash provided by (used in) operating activities
 
21,217 
 
25,971 
 
42,481 
 
70,241 
 
71,654 
 
112,706 
INVESTING ACTIVITIES:
Purchases of property and equipment
 
(12,479)  
(22,620)  
(38,141)  
(55,165)  
(54,733)  
(69,753) 
Proceeds from property and equipment sales and incentives
 
1,181 
 
1,342 
 
3,361 
 
3,559 
 
4,513 
 
4,794 
Acquisitions, net of cash acquired, non-marketable investments, and other
 
(1,629)  
(622)  
(5,458)  
(4,547)  
(6,289)  
(4,928) 
Sales and maturities of marketable securities
 
1,393 
 
8,069 
 
4,059 
 
12,726 
 
9,742 
 
14,294 
Purchases of marketable securities
 
(219)  
(3,068)  
(1,053)  
(13,472)  
(1,286)  
(13,907) 
Net cash provided by (used in) investing activities
 
(11,753)  
(16,899)  
(37,232)  
(56,899)  
(48,053)  
(69,500) 
FINANCING ACTIVITIES:
Proceeds from short-term debt, and other
 
216 
 
1,725 
 
17,395 
 
2,588 
 
28,002 
 
3,322 
Repayments of short-term debt, and other
 
(8,095)  
(1,820)  
(19,339)  
(2,453)  
(35,136)  
(8,791) 
Proceeds from long-term debt
 
— 
 
— 
 
— 
 
— 
 
8,235 
 
— 
Repayments of long-term debt
 
— 
 
(2,183)  
(3,386)  
(6,682)  
(4,643)  
(6,972) 
Principal repayments of finance leases
 
(1,005)  
(402)  
(3,605)  
(1,710)  
(5,245)  
(2,489) 
Principal repayments of financing obligations
 
(64)  
(78)  
(198)  
(247)  
(260)  
(320) 
Net cash provided by (used in) financing activities
 
(8,948)  
(2,758)  
(9,133)  
(8,504)  
(9,047)  
(15,250) 
Foreign currency effect on cash, cash equivalents, and restricted cash
 
(502)  
690 
 
(288)  
(51)  
349 
 
640 
Net increase (decrease) in cash, cash equivalents, and restricted cash
 
14 
 
7,004 
 
(4,172)  
4,787 
 
14,903 
 
28,596 
CASH, CASH EQUIVALENTS, AND RESTRICTED CASH, END OF PERIOD
$ 
50,081 
$ 
78,677 
$ 
50,081 
$ 
78,677 
$ 
50,081 
$ 
78,677 
SUPPLEMENTAL CASH FLOW INFORMATION:
Cash paid for interest on debt, net of capitalized interest
$ 
465 
$ 
266 
$ 
1,821 
$ 
1,215 
$ 
2,450 
$ 
2,002 
Cash paid for operating leases
 
2,692 
 
2,940 
 
7,687 
 
9,116 
 
10,052 
 
11,882 
Cash paid for interest on finance leases
 
76 
 
71 
 
234 
 
217 
 
318 
 
291 
Cash paid for interest on financing obligations
 
50 
 
47 
 
150 
 
161 
 
205 
 
207 
Cash paid for income taxes, net of refunds
 
2,628 
 
2,004 
 
6,982 
 
8,162 
 
8,677 
 
12,359 
Assets acquired under operating leases
 
3,345 
 
3,571 
 
11,075 
 
11,235 
 
15,844 
 
14,212 
Property and equipment acquired under finance leases, net of remeasurements and 
modifications
 
183 
 
186 
 
431 
 
409 
 
748 
 
620 
Property and equipment recognized during the construction period of build-to-suit 
lease arrangements
 
93 
 
21 
 
308 
 
89 
 
618 
 
138 
Property and equipment derecognized after the construction period of build-to-suit 
lease arrangements, with the associated leases recognized as operating
 
492 
 
— 
 
1,212 
 
— 
 
3,063 
 
162', Similarity: 0.03047819232790331

Highlight Summary:
Total Highlights: 524
Highlighted: True
INFO:     171.76.84.160:5191 - "POST /query-pdf/ HTTP/1.1" 200 OK
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.22:42070 - "GET /favicon.ico HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.22:43818 - "GET /manage/account/login HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.22:46342 - "GET /admin/index.html HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.22:59246 - "GET /index.html HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.22:43270 - "GET /%2BCSCOE%2B/logon.html HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.22:54478 - "GET /cgi-bin/login.cgi HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.22:52070 - "GET /logon.htm HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.22:58804 - "GET /login.jsp HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.22:45800 - "GET /doc/index.html HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     104.234.115.22:36296 - "GET / HTTP/1.1" 404 Not Found
INFO:     185.191.126.248:60564 - "GET / HTTP/1.1" 404 Not Found
INFO:     154.213.184.18:36344 - "CONNECT example.com%3A443 HTTP/1.1" 404 Not Found
INFO:     154.213.184.18:43374 - "CONNECT example.com%3A443 HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     198.235.24.199:62716 - "GET / HTTP/1.1" 404 Not Found
INFO:     154.213.184.18:40890 - "CONNECT example.com%3A443 HTTP/1.1" 404 Not Found
INFO:     199.45.155.105:35518 - "GET / HTTP/1.1" 404 Not Found
INFO:     199.45.155.105:46564 - "GET / HTTP/1.1" 404 Not Found
INFO:     199.45.155.105:46586 - "PRI %2A HTTP/2.0" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     185.191.126.248:41724 - "GET / HTTP/1.1" 404 Not Found
INFO:     52.228.154.220:58548 - "GET / HTTP/1.1" 404 Not Found
INFO:     154.213.184.18:57158 - "CONNECT example.com%3A443 HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
INFO:     165.232.45.225:47290 - "GET / HTTP/1.1" 404 Not Found
INFO:     165.232.45.225:47304 - "GET /login HTTP/1.1" 404 Not Found
INFO:     206.168.34.43:35706 - "GET / HTTP/1.1" 404 Not Found
INFO:     206.168.34.43:43554 - "GET / HTTP/1.1" 404 Not Found
INFO:     206.168.34.43:43578 - "PRI %2A HTTP/2.0" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     180.149.126.13:31541 - "GET /c/ HTTP/1.1" 404 Not Found
INFO:     154.213.184.18:50372 - "CONNECT example.com%3A443 HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     74.82.47.32:28217 - "GET / HTTP/1.1" 404 Not Found
INFO:     74.82.47.12:34113 - "GET /favicon.ico HTTP/1.1" 404 Not Found
INFO:     74.82.47.48:18227 - "GET http%3A//api.ipify.org/?format=json HTTP/1.1" 404 Not Found
INFO:     74.82.47.52:37879 - "CONNECT www.shadowserver.org%3A443 HTTP/1.1" 404 Not Found
INFO:     154.213.184.18:43772 - "CONNECT example.com%3A443 HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     185.191.126.248:37500 - "GET / HTTP/1.1" 404 Not Found
INFO:     154.213.184.18:52748 - "CONNECT example.com%3A443 HTTP/1.1" 404 Not Found
INFO:     180.149.125.163:43101 - "GET / HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
INFO:     167.94.145.111:35796 - "GET / HTTP/1.1" 404 Not Found
INFO:     167.94.145.111:49630 - "GET / HTTP/1.1" 404 Not Found
INFO:     167.94.145.111:49670 - "PRI %2A HTTP/2.0" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     154.213.184.18:60268 - "CONNECT example.com%3A443 HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
INFO:     165.154.162.212:55420 - "GET / HTTP/1.1" 404 Not Found
INFO:     165.154.162.212:55730 - "GET /favicon.ico HTTP/1.1" 404 Not Found
INFO:     165.154.162.212:56032 - "GET /sitemap.xml HTTP/1.1" 404 Not Found
INFO:     165.154.162.212:56030 - "GET /robots.txt HTTP/1.1" 404 Not Found
INFO:     154.213.184.18:41538 - "CONNECT example.com%3A443 HTTP/1.1" 404 Not Found
INFO:     31.220.1.88:33176 - "HEAD / HTTP/1.0" 404 Not Found
INFO:     185.247.137.5:49099 - "GET / HTTP/1.1" 404 Not Found
INFO:     31.220.1.88:57784 - "HEAD / HTTP/1.0" 404 Not Found
WARNING:  Invalid HTTP request received.
INFO:     154.213.184.18:52744 - "CONNECT example.com%3A443 HTTP/1.1" 404 Not Found
INFO:     154.213.184.18:60390 - "CONNECT example.com%3A443 HTTP/1.1" 404 Not Found
INFO:     74.82.47.11:3511 - "GET / HTTP/1.1" 404 Not Found
INFO:     74.82.47.43:53783 - "GET /favicon.ico HTTP/1.1" 404 Not Found
INFO:     74.82.47.43:57459 - "GET http%3A//api.ipify.org/?format=json HTTP/1.1" 404 Not Found
INFO:     74.82.47.23:15241 - "CONNECT www.shadowserver.org%3A443 HTTP/1.1" 404 Not Found
INFO:     47.251.104.164:50756 - "GET /.env HTTP/1.1" 404 Not Found
INFO:     37.59.165.86:57595 - "GET / HTTP/1.1" 404 Not Found
INFO:     37.59.165.83:47089 - "GET /favicon.ico HTTP/1.1" 404 Not Found
INFO:     205.210.31.53:61096 - "GET / HTTP/1.1" 404 Not Found
INFO:     154.213.184.18:58792 - "CONNECT example.com%3A443 HTTP/1.1" 404 Not Found
WARNING:  Invalid HTTP request received.
WARNING:  Invalid HTTP request received.
INFO:     80.66.83.47:54626 - "CONNECT hotmail-com.olc.protection.outlook.com%3A25 HTTP/1.1" 404 Not Found
